{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving MNIST\n",
    "\n",
    "Problem statement: Predict digit value given handwriting picture (28 x 28 pixel)\n",
    "Type: Multiclass Classification (0-9 digit)\n",
    "Performance metric: Accuracy; Since number of samples each label is quite the same.\n",
    "\n",
    "The problem might be solved by these learning algorithms:\n",
    "1. Multiclass Classifier\n",
    "2. Ensemble Binary Classifier\n",
    "3. Multilayer Perceptron\n",
    "4. Convolutional Neural Network\n",
    "\n",
    "What I did in this notebook:\n",
    "1. Data pre-processing:\n",
    "    a. Flatten the data (for Multiclass Classifier, Ensemble Binary Classifier, Multilayer Perceptron).\n",
    "    b. Normalize the data so it could be faster to converge.\n",
    "2. Modelling:\n",
    "    a. Compare performance metric on cross-validation for each learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n",
      "/kaggle/input/digit-recognizer/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33600, 784), (33600,), (8400, 784), (8400,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file\n",
    "df_train = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n",
    "\n",
    "X_train = df_train.iloc[:,1:]\n",
    "y_train = df_train.iloc[:,0]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                    stratify = y_train,\n",
    "                                                    test_size=0.2, random_state = 41)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing\n",
    "In this section, I want to see samples of training data; and prepare the dataset so it could be fed into learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.09839285714285714,\n",
       " 1: 0.11151785714285714,\n",
       " 2: 0.09946428571428571,\n",
       " 3: 0.10360119047619047,\n",
       " 4: 0.09696428571428571,\n",
       " 5: 0.09035714285714286,\n",
       " 6: 0.09848214285714285,\n",
       " 7: 0.10479166666666667,\n",
       " 8: 0.09672619047619048,\n",
       " 9: 0.09970238095238096}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of samples per digit in training dataset\n",
    "# I need to make sure whether the dataset is balance or imbalance\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "proportions = counts/counts.sum()\n",
    "dict(zip(unique, proportions))\n",
    "\n",
    "# The samples are slightly well distributed. The proportion of each label between 9-10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See samples of the image\n",
    "def series_to_array(series):\n",
    "    data = np.array(series)\n",
    "    return data\n",
    "\n",
    "# Plot Digit\n",
    "def plot_digit(series):\n",
    "    data = series_to_array(series)\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plot_digit(X_train.iloc[10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33600, 784), (8400, 784))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "# To get faster time to converge, I need to scale the feature value to the value between 0-1.\n",
    "# Currently the value representation is between 0-255. So we need to divide it with 255.\n",
    "\n",
    "X_train_flat = X_train/255\n",
    "X_val_flat = X_val/255\n",
    "\n",
    "X_train_flat.shape, X_val_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling\n",
    "In this section, I will do following tasks:\n",
    "1. Prepare cross-validation function\n",
    "2. Define several models\n",
    "3. Compare performance metric of each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-fold being used in order to get stratified split\n",
    "skfolds = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# Calculate Accuracy using cross-validation\n",
    "def calculate_cv_accuracy(model, X, y):\n",
    "    acc = cross_val_score(model, X, y, cv=skfolds, scoring = \"accuracy\")\n",
    "    return acc\n",
    "    \n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "    for name, model in models:\n",
    "        cv_score = calculate_cv_accuracy(model, X_train, y_train)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_pred)\n",
    "        results.append((name, model, cv_score.mean(), cv_score.std(), test_acc))\n",
    "        msg = \"{}: Accuracy on CV {} ({}); Test {}\".format(name, cv_score.mean(), cv_score.std(), test_acc)\n",
    "        print(msg)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Classifier\n",
    "In this section, I use Logistic Regression and Random Forest to build multiclass classifier. These two learning algorithms have different approach to solve the problem and I want to see the result out of two different algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "logistic_multi = LogisticRegression(multi_class='multinomial') # solver sage & scaled feature are used to get faster converge time\n",
    "rf_multi = RandomForestClassifier(n_estimators = 50, random_state=41)\n",
    "\n",
    "# List of models\n",
    "models = []\n",
    "models.append((\"Logistic Multiclass\", logistic_multi))\n",
    "models.append((\"Random Forest Multiclass\", rf_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Multiclass: Accuracy on CV 0.9127976190476191 (0.0018959057510277476); Test 0.9166666666666666\n",
      "Random Forest Multiclass: Accuracy on CV 0.9561309523809524 (0.0005373651811091979); Test 0.9603571428571429\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "# The evaluation done in cross-validation and test dataset\n",
    "# So I could make sure the model is not overfitting the training data\n",
    "multiclass_result = evaluate_models(models, X_train_flat, y_train, X_val_flat, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Binary Classifiers\n",
    "Instead of creating one model to classify 10 labels, I want to create 10 models for each labels and ensemble them. The ensemble learning usually have better performance if it is constructed with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before creating the ensemble model,\n",
    "# We need to adjust the target for each binary classifiers first\n",
    "\n",
    "# Let's create target for binary classification\n",
    "y_train_0 = (y_train == 0)\n",
    "y_train_1 = (y_train == 1)\n",
    "y_train_2 = (y_train == 2)\n",
    "y_train_3 = (y_train == 3)\n",
    "y_train_4 = (y_train == 4)\n",
    "y_train_5 = (y_train == 5)\n",
    "y_train_6 = (y_train == 6)\n",
    "y_train_7 = (y_train == 7)\n",
    "y_train_8 = (y_train == 8)\n",
    "y_train_9 = (y_train == 9)\n",
    "# Now we have separated target for each digit\n",
    "\n",
    "# Lets define Binary Classifier for each digit\n",
    "# I will use two type binary classifier for each digit i.e. Logistic Regression and Random Forest\n",
    "rf_0 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_1 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_2 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_3 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_4 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_5 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_6 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_7 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_8 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "rf_9 = RandomForestClassifier(n_estimators = 2, random_state=41)\n",
    "\n",
    "logit_0 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_1 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_2 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_3 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_4 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_5 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_6 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_7 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_8 = LogisticRegression(l1_ratio= 0.5)\n",
    "logit_9 = LogisticRegression(l1_ratio= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression digit 0: Accuracy on CV 0.9909226190476191 (0.0008636082219373046);\n",
      "Logistic Regression digit 1: Accuracy on CV 0.991904761904762 (0.0004454354031873941);\n",
      "Logistic Regression digit 2: Accuracy on CV 0.9774999999999999 (0.0005693787639794287);\n",
      "Logistic Regression digit 3: Accuracy on CV 0.972202380952381 (0.0014161163396869245);\n",
      "Logistic Regression digit 4: Accuracy on CV 0.9822321428571428 (0.0013501543121683269);\n",
      "Logistic Regression digit 5: Accuracy on CV 0.9724404761904762 (0.0009951341367440683);\n",
      "Logistic Regression digit 6: Accuracy on CV 0.9856250000000001 (0.0015988542879241821);\n",
      "Logistic Regression digit 7: Accuracy on CV 0.9834226190476191 (0.0012570661953537248);\n",
      "Logistic Regression digit 8: Accuracy on CV 0.9593154761904762 (0.0014476650603589703);\n",
      "Logistic Regression digit 9: Accuracy on CV 0.9641071428571428 (0.0006479625314606278);\n",
      "Random Forest digit 0: Accuracy on CV 0.9792261904761904 (0.002757115613813932);\n",
      "Random Forest digit 1: Accuracy on CV 0.986875 (0.0012773371270163834);\n",
      "Random Forest digit 2: Accuracy on CV 0.960625 (0.0006479625314606635);\n",
      "Random Forest digit 3: Accuracy on CV 0.9505059523809525 (0.001625773843303315);\n",
      "Random Forest digit 4: Accuracy on CV 0.9619940476190476 (0.0012883845727329523);\n",
      "Random Forest digit 5: Accuracy on CV 0.9617261904761905 (0.0011785113019775915);\n",
      "Random Forest digit 6: Accuracy on CV 0.9758630952380952 (0.0006285330976768624);\n",
      "Random Forest digit 7: Accuracy on CV 0.9672321428571428 (0.0008019162848397008);\n",
      "Random Forest digit 8: Accuracy on CV 0.9498214285714286 (0.0019817942636412445);\n",
      "Random Forest digit 9: Accuracy on CV 0.9517261904761906 (0.0007338588098772529);\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each of binary classifiers\n",
    "binary_models_rf = []\n",
    "binary_models_rf.append((\"Random Forest digit 0\", rf_0))\n",
    "binary_models_rf.append((\"Random Forest digit 1\", rf_1))\n",
    "binary_models_rf.append((\"Random Forest digit 2\", rf_2))\n",
    "binary_models_rf.append((\"Random Forest digit 3\", rf_3))\n",
    "binary_models_rf.append((\"Random Forest digit 4\", rf_4))\n",
    "binary_models_rf.append((\"Random Forest digit 5\", rf_5))\n",
    "binary_models_rf.append((\"Random Forest digit 6\", rf_6))\n",
    "binary_models_rf.append((\"Random Forest digit 7\", rf_7))\n",
    "binary_models_rf.append((\"Random Forest digit 8\", rf_8))\n",
    "binary_models_rf.append((\"Random Forest digit 9\", rf_9))\n",
    "\n",
    "binary_models = []\n",
    "binary_models.append((\"Logistic Regression digit 0\", logit_0))\n",
    "binary_models.append((\"Logistic Regression digit 1\", logit_1))\n",
    "binary_models.append((\"Logistic Regression digit 2\", logit_2))\n",
    "binary_models.append((\"Logistic Regression digit 3\", logit_3))\n",
    "binary_models.append((\"Logistic Regression digit 4\", logit_4))\n",
    "binary_models.append((\"Logistic Regression digit 5\", logit_5))\n",
    "binary_models.append((\"Logistic Regression digit 6\", logit_6))\n",
    "binary_models.append((\"Logistic Regression digit 7\", logit_7))\n",
    "binary_models.append((\"Logistic Regression digit 8\", logit_8))\n",
    "binary_models.append((\"Logistic Regression digit 9\", logit_9))\n",
    "\n",
    "dict_y_train = {'y_train_0': y_train_0,\n",
    "               'y_train_1': y_train_1,\n",
    "               'y_train_2': y_train_2,\n",
    "               'y_train_3': y_train_3,\n",
    "               'y_train_4': y_train_4,\n",
    "               'y_train_5': y_train_5,\n",
    "               'y_train_6': y_train_6,\n",
    "               'y_train_7': y_train_7,\n",
    "               'y_train_8': y_train_8,\n",
    "               'y_train_9': y_train_9\n",
    "               }\n",
    "i = 0\n",
    "for name, model in binary_models:\n",
    "    y = (\"y_train_\" + str(i))\n",
    "    cv_score = calculate_cv_accuracy(model, X_train_flat, dict_y_train[y])\n",
    "    msg = \"{}: Accuracy on CV {} ({});\".format(name, cv_score.mean(), cv_score.std())\n",
    "    i += 1\n",
    "    print(msg)\n",
    "\n",
    "i = 0\n",
    "for name, model in binary_models_rf:\n",
    "    y = (\"y_train_\" + str(i))\n",
    "    cv_score = calculate_cv_accuracy(model, X_train_flat, dict_y_train[y])\n",
    "    msg = \"{}: Accuracy on CV {} ({});\".format(name, cv_score.mean(), cv_score.std())\n",
    "    i += 1\n",
    "    print(msg)    \n",
    "    \n",
    "# We now have different binary classifiers for each label with different performance\n",
    "# The accuracy of each binary classifiers ranging from  95% - 99%\n",
    "# I am afraid that each classifiers is overfitting the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=0.5, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit each of binary classifiers\n",
    "rf_0.fit(X_train_flat, y_train_0)\n",
    "rf_1.fit(X_train_flat, y_train_1)\n",
    "rf_2.fit(X_train_flat, y_train_2)\n",
    "rf_3.fit(X_train_flat, y_train_3)\n",
    "rf_4.fit(X_train_flat, y_train_4)\n",
    "rf_5.fit(X_train_flat, y_train_5)\n",
    "rf_6.fit(X_train_flat, y_train_6)\n",
    "rf_7.fit(X_train_flat, y_train_7)\n",
    "rf_8.fit(X_train_flat, y_train_8)\n",
    "rf_9.fit(X_train_flat, y_train_9)\n",
    "\n",
    "logit_0.fit(X_train_flat, y_train_0)\n",
    "logit_1.fit(X_train_flat, y_train_1)\n",
    "logit_2.fit(X_train_flat, y_train_2)\n",
    "logit_3.fit(X_train_flat, y_train_3)\n",
    "logit_4.fit(X_train_flat, y_train_4)\n",
    "logit_5.fit(X_train_flat, y_train_5)\n",
    "logit_6.fit(X_train_flat, y_train_6)\n",
    "logit_7.fit(X_train_flat, y_train_7)\n",
    "logit_8.fit(X_train_flat, y_train_8)\n",
    "logit_9.fit(X_train_flat, y_train_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for the second layer model\n",
    "# The training data is the result of prediction of each models in first layer\n",
    "# I'll put probability prediciton from Random Forest and Logistic Regression\n",
    "\n",
    "def generate_second_layer_data(X):\n",
    "\n",
    "    X2_pred_0 = rf_0.predict_proba(X)\n",
    "    X2_pred_0  = [item[1] for item in X2_pred_0]\n",
    "    X2_pred_1 = rf_1.predict_proba(X)\n",
    "    X2_pred_1  = [item[1] for item in X2_pred_1]\n",
    "    X2_pred_2 = rf_2.predict_proba(X)\n",
    "    X2_pred_2  = [item[1] for item in X2_pred_2]\n",
    "    X2_pred_3 = rf_3.predict_proba(X)\n",
    "    X2_pred_3  = [item[1] for item in X2_pred_3]\n",
    "    X2_pred_4 = rf_4.predict_proba(X)\n",
    "    X2_pred_4  = [item[1] for item in X2_pred_4]\n",
    "    X2_pred_5 = rf_5.predict_proba(X)\n",
    "    X2_pred_5  = [item[1] for item in X2_pred_5]\n",
    "    X2_pred_6 = rf_6.predict_proba(X)\n",
    "    X2_pred_6  = [item[1] for item in X2_pred_6]\n",
    "    X2_pred_7 = rf_7.predict_proba(X)\n",
    "    X2_pred_7  = [item[1] for item in X2_pred_7]\n",
    "    X2_pred_8 = rf_8.predict_proba(X)\n",
    "    X2_pred_8  = [item[1] for item in X2_pred_8]\n",
    "    X2_pred_9 = rf_9.predict_proba(X)\n",
    "    X2_pred_9  = [item[1] for item in X2_pred_9]\n",
    "\n",
    "    X2_log_pred_0 = logit_0.predict_proba(X)\n",
    "    X2_log_pred_0  = [item[1] for item in X2_log_pred_0]\n",
    "    X2_log_pred_1 = logit_1.predict_proba(X)\n",
    "    X2_log_pred_1  = [item[1] for item in X2_log_pred_1]\n",
    "    X2_log_pred_2 = logit_2.predict_proba(X)\n",
    "    X2_log_pred_2  = [item[1] for item in X2_log_pred_2]\n",
    "    X2_log_pred_3 = logit_3.predict_proba(X)\n",
    "    X2_log_pred_3  = [item[1] for item in X2_log_pred_3]\n",
    "    X2_log_pred_4 = logit_4.predict_proba(X)\n",
    "    X2_log_pred_4  = [item[1] for item in X2_log_pred_4]\n",
    "    X2_log_pred_5 = logit_5.predict_proba(X)\n",
    "    X2_log_pred_5  = [item[1] for item in X2_log_pred_5]\n",
    "    X2_log_pred_6 = logit_6.predict_proba(X)\n",
    "    X2_log_pred_6  = [item[1] for item in X2_log_pred_6]\n",
    "    X2_log_pred_7 = logit_7.predict_proba(X)\n",
    "    X2_log_pred_7  = [item[1] for item in X2_log_pred_7]\n",
    "    X2_log_pred_8 = logit_8.predict_proba(X)\n",
    "    X2_log_pred_8  = [item[1] for item in X2_log_pred_8]\n",
    "    X2_log_pred_9 = logit_9.predict_proba(X)\n",
    "    X2_log_pred_9  = [item[1] for item in X2_log_pred_9]\n",
    "\n",
    "    # Create dataframe of all the result\n",
    "    data = {'X2_zero':X2_pred_0,\n",
    "            'X2_log_zero':X2_log_pred_0,\n",
    "            'X2_one':X2_pred_1,\n",
    "            'X2_log_one':X2_log_pred_1,\n",
    "            'X2_two':X2_pred_2,\n",
    "            'X2_log_two':X2_log_pred_2,\n",
    "            'X2_three':X2_pred_3,\n",
    "            'X2_log_three':X2_log_pred_3,\n",
    "            'X2_four':X2_pred_4,\n",
    "            'X2_log_four':X2_log_pred_4,\n",
    "            'X2_five':X2_pred_5,\n",
    "            'X2_log_five':X2_log_pred_5,\n",
    "            'X2_six':X2_pred_6,\n",
    "            'X2_log_six':X2_log_pred_6,\n",
    "            'X2_seven':X2_pred_7,\n",
    "            'X2_log_seven':X2_log_pred_7,\n",
    "            'X2_eight':X2_pred_8, \n",
    "            'X2_log_eight':X2_log_pred_8, \n",
    "            'X2_nine':X2_pred_9,\n",
    "            'X2_log_nine':X2_log_pred_9\n",
    "           }\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X2_zero</th>\n",
       "      <th>X2_log_zero</th>\n",
       "      <th>X2_one</th>\n",
       "      <th>X2_log_one</th>\n",
       "      <th>X2_two</th>\n",
       "      <th>X2_log_two</th>\n",
       "      <th>X2_three</th>\n",
       "      <th>X2_log_three</th>\n",
       "      <th>X2_four</th>\n",
       "      <th>X2_log_four</th>\n",
       "      <th>X2_five</th>\n",
       "      <th>X2_log_five</th>\n",
       "      <th>X2_six</th>\n",
       "      <th>X2_log_six</th>\n",
       "      <th>X2_seven</th>\n",
       "      <th>X2_log_seven</th>\n",
       "      <th>X2_eight</th>\n",
       "      <th>X2_log_eight</th>\n",
       "      <th>X2_nine</th>\n",
       "      <th>X2_log_nine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.233164e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959978e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.679473e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.601034e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.208717e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.448909e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.138870e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.723117e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.708642e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.578087e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.874257e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.662300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.144153e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.350213e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.036980e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.397891e-05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.229674e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.283297e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.489376e-08</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.850726e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.254883e-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.485930e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.761544e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.251917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.631009e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.998658e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.822449e-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.566109e-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.531861e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.185804e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.040739e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X2_zero   X2_log_zero  X2_one    X2_log_one  X2_two  X2_log_two  X2_three  \\\n",
       "0      0.0  3.233164e-08     0.0  5.959978e-03     0.0    0.000064       0.0   \n",
       "1      0.0  2.138870e-07     0.0  6.723117e-07     0.0    0.000070       0.0   \n",
       "2      0.0  5.144153e-06     0.0  5.350213e-12     1.0    0.999547       0.0   \n",
       "3      0.0  6.489376e-08     0.5  2.850726e-03     0.0    0.000192       0.0   \n",
       "4      1.0  9.998658e-01     0.0  1.822449e-13     0.0    0.000233       0.0   \n",
       "\n",
       "   X2_log_three  X2_four   X2_log_four  X2_five  X2_log_five  X2_six  \\\n",
       "0      0.347978      0.0  1.679473e-03      0.0     0.004894     0.0   \n",
       "1      0.000132      0.0  3.708642e-02      0.0     0.038983     0.0   \n",
       "2      0.019313      0.0  1.036980e-06      0.0     0.000198     0.0   \n",
       "3      0.000180      0.0  2.254883e-04      1.0     0.852286     0.0   \n",
       "4      0.000168      0.0  1.566109e-09      0.0     0.002296     0.0   \n",
       "\n",
       "     X2_log_six  X2_seven  X2_log_seven  X2_eight  X2_log_eight  X2_nine  \\\n",
       "0  1.601034e-06       0.0  5.208717e-03       0.0      0.026889      1.0   \n",
       "1  6.578087e-05       0.0  4.874257e-03       0.0      0.011756      1.0   \n",
       "2  4.397891e-05       0.5  2.229674e-05       0.0      0.000036      0.0   \n",
       "3  3.485930e-07       0.0  3.761544e-05       0.0      0.251917      0.0   \n",
       "4  2.531861e-08       0.0  3.185804e-12       0.0      0.000025      0.0   \n",
       "\n",
       "    X2_log_nine  \n",
       "0  1.448909e-01  \n",
       "1  9.662300e-01  \n",
       "2  3.283297e-09  \n",
       "3  3.631009e-06  \n",
       "4  3.040739e-09  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See samples of prediction result\n",
    "df_X2 = generate_second_layer_data(X_train_flat)\n",
    "df_X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model for second layer in stacking\n",
    "# I will use also random forest and training it using the predictions of several models in first layer\n",
    "rf_final = RandomForestClassifier(n_estimators = 10, random_state=41)\n",
    "logistic_final = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "\n",
    "rf_final.fit(df_X2, y_train)\n",
    "logistic_final.fit(df_X2, y_train)\n",
    "\n",
    "stacking_models = []\n",
    "stacking_models.append(('Random Forest 2nd layer', rf_final))\n",
    "stacking_models.append(('Logistic Regression 2nd layer', logistic_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 2nd layer: Accuracy on Test 0.9185714285714286;\n",
      "Logistic Regression 2nd layer: Accuracy on Test 0.924047619047619;\n"
     ]
    }
   ],
   "source": [
    "# Prediction in Stacking model\n",
    "# I will use test dataset to see whether the stacking model will perform better or not\n",
    "df_test_X2 = generate_second_layer_data(X_val_flat)\n",
    "\n",
    "for name, model in stacking_models:\n",
    "    y_pred = model.predict(df_test_X2)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    msg = \"{}: Accuracy on Test {};\".format(name, acc)\n",
    "    print(msg)\n",
    "    \n",
    "# It turns out that our stacking model do not outperform the multiclass classifiers earlier\n",
    "# The multiclass classifier has 0.9677 accuracy on test dataset\n",
    "# The ensemble binary classifier has 0.9461 accuracy on test dataset\n",
    "# Having these finding, We learn that multiclass random forest classifier perform better on the MNIST dataset\n",
    "# I believe that the stacking model is overfitting the training dataset,\n",
    "# Supporteb by the fact that each binary classifiers have really high accuracy on training dataset, but not in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron\n",
    "In this section I will approach the problem using Multilayer Perceptron (MLP) or Neural Network . The implementation will be done using keras framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the X_train and X_test\n",
    "# So it could be in range of 0-1\n",
    "# It will help MLP to converge faster\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_val = X_val / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the keras only take input of array, I need to convert dataframe to array\n",
    "arr_X_train = X_train.to_numpy()\n",
    "arr_X_val = X_val.to_numpy()\n",
    "arr_y_train = y_train.to_numpy()\n",
    "arr_y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I need to define the model first\n",
    "# I will use 2 hidden layers in this model and 384/256 units for each layers\n",
    "# I will use elu activation and set he initialization of each weight\n",
    "# L1 regularization is applied to get faster convergence time in sparse dataset like MNIST\n",
    "\n",
    "mlp_model = keras.Sequential([\n",
    "#     keras.layers.Flatten(input_shape = (28,28)),\n",
    "    keras.layers.Dense(384, activation= 'relu'\n",
    "                       , kernel_initializer= 'he_normal'\n",
    "                       , kernel_regularizer= keras.regularizers.l1(0.00001)\n",
    "                      ),\n",
    "    keras.layers.Dense(256, activation= 'relu'\n",
    "                       , kernel_initializer= 'he_normal'\n",
    "                       , kernel_regularizer= keras.regularizers.l1(0.0001)\n",
    "                      ),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "mlp_model.compile(optimizer= 'adam',\n",
    "                  loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics= ['accuracy']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples\n",
      "Epoch 1/8\n",
      "33600/33600 [==============================] - 4s 128us/sample - loss: 0.6792 - accuracy: 0.9238\n",
      "Epoch 2/8\n",
      "33600/33600 [==============================] - 3s 87us/sample - loss: 0.3544 - accuracy: 0.9651\n",
      "Epoch 3/8\n",
      "33600/33600 [==============================] - 3s 86us/sample - loss: 0.2574 - accuracy: 0.9732\n",
      "Epoch 4/8\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.2097 - accuracy: 0.9782\n",
      "Epoch 5/8\n",
      "33600/33600 [==============================] - 3s 87us/sample - loss: 0.1782 - accuracy: 0.9824\n",
      "Epoch 6/8\n",
      "33600/33600 [==============================] - 3s 88us/sample - loss: 0.1525 - accuracy: 0.9858\n",
      "Epoch 7/8\n",
      "33600/33600 [==============================] - 3s 85us/sample - loss: 0.1339 - accuracy: 0.9883\n",
      "Epoch 8/8\n",
      "33600/33600 [==============================] - 3s 89us/sample - loss: 0.1225 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80eb8e2358>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "mlp_model.fit(arr_X_train, arr_y_train, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy on Test: 0.9770237803459167\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model in test dataset\n",
    "test_loss, test_accuracy = mlp_model.evaluate(arr_X_val,arr_y_val, verbose= 0)\n",
    "print(\"MLP Accuracy on Test: {}\".format(test_accuracy))\n",
    "\n",
    "# It turns out that MLP model do outperform the multiclass classifiers earlier\n",
    "# It has 0.98 accuracy on test dataset! 0.0073 better!\n",
    "# The multiclass classifier has 0.9677 accuracy on test dataset\n",
    "# The ensemble binary classifier has 0.9461 accuracy on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction\n",
    "# Prediction for submission\n",
    "arr_X_test = df_test.to_numpy()\n",
    "arr_X_test = arr_X_test / 255\n",
    "arr_y_pred = mlp_model.predict(arr_X_test)\n",
    "\n",
    "y_pred_mlp = []\n",
    "for i in range(len(arr_y_pred)):\n",
    "    y_pred_mlp.append(np.argmax(arr_y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file submisssion from MLP\n",
    "submission_mlp = pd.read_csv(\"/kaggle/input/digit-recognizer/sample_submission.csv\")\n",
    "submission_mlp.iloc[:,1] = (y_pred_mlp)\n",
    "submission_mlp.to_csv(\"submission_mlp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network (CNN)\n",
    "In this section I will approach the problem using Convolutional Neural Network (CNN). The difference with pervious MLP is instead of connecting all feature to all units in layer, it selects only particular visual receptor. The CNN often use in visual applications such as object detection and semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33600, 28, 28, 1),\n",
       " (33600, 10),\n",
       " (8400, 28, 28, 1),\n",
       " (8400, 10),\n",
       " (28000, 28, 28, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare dataset for CNN\n",
    "\n",
    "# Get data from previous model\n",
    "arr_X_train = X_train.to_numpy()\n",
    "arr_X_val = X_val.to_numpy()\n",
    "arr_y_train = y_train.to_numpy()\n",
    "arr_y_val = y_val.to_numpy()\n",
    "\n",
    "# Get data from csv file and normalize it\n",
    "arr_X_test = df_test.to_numpy()\n",
    "arr_X_test = arr_X_test / 255\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# Reshape the array to (28,28,1)\n",
    "X_train = arr_X_train.reshape(arr_X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_val = arr_X_val.reshape(arr_X_val.shape[0], img_rows, img_cols, 1)\n",
    "X_test = arr_X_test.reshape(arr_X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN architecture\n",
    "# The model is constructed by several layers\n",
    "# I use relu activation and he initializer to get faster converge time\n",
    "# The first layer is convolutional layer with 16 filter, 3x3 kernel size, 1 stride\n",
    "# The second layer is the same with the first layer. They are supposed to capture feature maps from digit image.\n",
    "# The third layer is pooling with maximum aggregation. It is used to reduce the size of feature maps.\n",
    "# The fourth layer is convolutional layer for the result from maximum pooling.\n",
    "# The next layer is dense neural network with 256 units, before feeding this layer with data I need to flatten first the data\n",
    "# The last layer / output layer is dense neural network with 10 units (the same with number of class)\n",
    "\n",
    "cnn_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=input_shape, kernel_initializer= 'he_normal'),\n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer= 'he_normal'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer= 'he_normal'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu', kernel_initializer= 'he_normal'),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "cnn_model.compile(optimizer= 'adam',\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics= ['accuracy']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples\n",
      "Epoch 1/3\n",
      "33600/33600 [==============================] - 7s 215us/sample - loss: 0.1790 - accuracy: 0.9451\n",
      "Epoch 2/3\n",
      "33600/33600 [==============================] - 4s 118us/sample - loss: 0.0558 - accuracy: 0.9825\n",
      "Epoch 3/3\n",
      "33600/33600 [==============================] - 5s 134us/sample - loss: 0.0395 - accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80b96ed2e8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "cnn_model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Accuracy on Test: 0.9851190447807312\n"
     ]
    }
   ],
   "source": [
    "## Evaluate model in test dataset\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_val, y_val, verbose= 0)\n",
    "print(\"CNN Accuracy on Test: {}\".format(test_accuracy))\n",
    "\n",
    "# It turns out that CNN model do outperform previos models\n",
    "# It has 0.99 accuracy on test dataset!!! 0.01 better than MLP. It is huge improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for submission\n",
    "arr_y_pred = cnn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of prediction\n",
    "y_pred = []\n",
    "for i in range(len(arr_y_pred)):\n",
    "    y_pred.append(np.argmax(arr_y_pred[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file submisssion from CNN\n",
    "submission_cnn = pd.read_csv(\"/kaggle/input/digit-recognizer/sample_submission.csv\")\n",
    "submission_cnn.iloc[:,1] = (y_pred)\n",
    "submission_cnn.to_csv(\"submission_cnn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
