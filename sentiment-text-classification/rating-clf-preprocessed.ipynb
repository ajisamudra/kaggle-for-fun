{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Library\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport re\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest\n\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import chi2, f_classif\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read files\ntrain = pd.read_csv(\"../input/shopee-sentiment-analysis/train.csv\")\ntest_df = pd.read_csv(\"../input/shopee-sentiment-analysis/test.csv\")\nadd_train = pd.read_csv(\"../input/chineseenglishfasttext/790393_1357444_compressed_test_labelled.csv/test_labelled.csv\")\n\n# concat train add_train\ntrain = pd.concat([train, add_train], axis=0)\n\n# shuffle rows\ntrain_df = train.sample(frac=1.).reset_index(drop=True)\n\ntrain_df.drop('review_id', axis=1, inplace=True)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import emoji  # https://pypi.org/project/emoji/\n\nhave_emoji_train_idx = []\nhave_emoji_test_idx = []\n\nfor idx, review in enumerate(train_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_test_idx.append(idx)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_emoji_percentage = round(len(have_emoji_train_idx) / train_df.shape[0] * 100, 2)\nprint(f'Train data has {len(have_emoji_train_idx)} rows that used emoji, that means {train_emoji_percentage} percent of the total')\n\ntest_emoji_percentage = round(len(have_emoji_test_idx) / test_df.shape[0] * 100, 2)\nprint(f'Test data has {len(have_emoji_test_idx)} rows that used emoji, that means {test_emoji_percentage} percent of the total')","execution_count":4,"outputs":[{"output_type":"stream","text":"Train data has 28566 rows that used emoji, that means 13.62 percent of the total\nTest data has 7582 rows that used emoji, that means 12.55 percent of the total\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def emoji_cleaning(text):\n    \n    # Change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # Delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text\n\ntrain_df_original = train_df.copy()\ntest_df_original = test_df.copy()\n\n# emoji_cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'] = train_df.loc[have_emoji_train_idx, 'review'].apply(emoji_cleaning)\ntest_df.loc[have_emoji_test_idx, 'review'] = test_df.loc[have_emoji_test_idx, 'review'].apply(emoji_cleaning)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_cleaning(text):\n    \n    # lowercase and delete newline\n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n    # delete punctuation\n    text = re.sub('[^a-z0-9 ]', ' ', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer])\n\ntrain_df['review'] = train_df['review'].apply(review_cleaning)\ntest_df['review'] = test_df['review'].apply(review_cleaning)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"repeated_rows_train = []\nrepeated_rows_test = []\n\nfor idx, review in enumerate(train_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_train.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_test.append(idx)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def delete_repeated_char(text):\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)    \n    return text\n\ntrain_df.loc[repeated_rows_train, 'review'] = train_df.loc[repeated_rows_train, 'review'].apply(delete_repeated_char)\ntest_df.loc[repeated_rows_test, 'review'] = test_df.loc[repeated_rows_test, 'review'].apply(delete_repeated_char)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recover_shortened_words(text):\n    \n    # put \\b (boundary) for avoid the characters in the word to be replaced\n    # I only make a few examples here, you can add if you're interested :)\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    \n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    \n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    \n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    \n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    \n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    \n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    \n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    \n    text = re.sub(r'\\borg\\b', 'orang', text)\n    \n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    \n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    \n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    \n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    \n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    \n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    \n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df['review'] = train_df['review'].apply(recover_shortened_words)","execution_count":10,"outputs":[{"output_type":"stream","text":"CPU times: user 37.8 s, sys: 13.4 ms, total: 37.9 s\nWall time: 37.9 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train['review'], train['rating'], test_size=0.15, stratify=train['rating'], random_state=101 )\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"((178269,), (31460,), (178269,), (31460,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation function\ndef evaluate_metrics(y_val, y_pred):\n    acc = accuracy_score(y_val, y_pred)\n    f1_macro = f1_score(y_val, y_pred, average='macro')\n    f1_micro= f1_score(y_val, y_pred, average='micro')\n    \n    print(\"Validation Score\")\n    print(\"Accuracy :\", acc)\n    print(\"F1 Macro :\", f1_macro)\n    print(\"F1 Micro :\", f1_micro)\n    \ndef evaluate_proba_metrics(y_val, y_pred):\n    auc = roc_auc_score(y_val, y_pred)\n    \n    print(\"Validation Score\")\n    print(\"ROC AUC :\", auc)\n    \ndef training_evaluate(model, X_train, X_val, y_train, y_val):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    \n    evaluate_metrics(y_val, y_pred)\n    return model, y_pred\n\n# the threshold for prediction is 0.31","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1 / train_df.rating.value_counts(normalize=True)\n# wgts = {1: 0.100706,\n#         2: 0.086540,\n#         3: 0.244811,\n#         4: 0.285163,\n#         5: 0.282779}\n# wgts","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"4     5.260145\n5     5.304492\n3     6.127172\n1    14.894820\n2    17.332975\nName: rating, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"wgts = {1: 9.929880,\n        2: 11.555317,\n        3: 4.084781,\n        4: 3.506763,\n        5: 3.536328}\nwgts","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"{1: 9.92988, 2: 11.555317, 3: 4.084781, 4: 3.506763, 5: 3.536328}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline model + preprocessed + cleaning\n# plain TfIdfVectorizer\n\nvectorizer = TfidfVectorizer(ngram_range=(1,5))\nvectorizer.fit(X_train)\n\ntrain_vct = vectorizer.transform(X_train)\nval_vct = vectorizer.transform(X_val)\n\n# Model\nmodel = LogisticRegression(C=0.9, class_weight=wgts)\n\n# Train and Evaluate\nlr, y_pred = training_evaluate(model, train_vct, val_vct, y_train, y_val)\n# ngram 1,2 0.538270820089002\n# ngram 1,5 0.562555626191989 norm l2\n# ngram 1,5 selectkbest 5000 chi2 0.483566433\n# ngram 1,5 selectkbest 10000 chi2 0.483566433 -> not works selectkbest\n# regularization strength 0.4 0.55\n# regularization strength 0.1 not work\n# add weight class 1 0.56303\n# add weight class 1.5 0.558963\n# add weight class 1 C 0.9 0.5685632\n# add weight class 1 C 0.95 0.5652574\n# add weight class 1 C 0.85 0.564589","execution_count":29,"outputs":[{"output_type":"stream","text":"Validation Score\nAccuracy : 0.5685632549268913\nF1 Macro : 0.5800024764978338\nF1 Micro : 0.5685632549268913\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vct = vectorizer.transform(test_df['review'])\ny_pred = lr.predict(test_vct)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_error = pd.concat([y_val, pd.Series(y_pred, index=y_val.index, name='prediction')], axis=1)\npd.pivot_table(df_error, index='prediction', columns='rating', values='rating', aggfunc=len) \n# df_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(df_error, index='rating', columns='prediction', values='rating', aggfunc=len) / df_error.prediction.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(df_error, index='rating', columns='prediction', values='rating', aggfunc=len) / df_error.prediction.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_y_test.rating.value_counts(normalize=True)\n\n# vanila tfidf & logreg in subset training \n# 5    0.346832\n# 4    0.337713\n# 3    0.198074\n# 1    0.083721\n# 2    0.033660\n\n# two steps prediction\n# 5    0.350506\n# 4    0.328032\n# 3    0.204975\n# 1    0.081884\n# 2    0.034604\n\n# fasttext tuned\n# 4    0.419696\n# 3    0.263690\n# 5    0.185166\n# 1    0.079501\n# 2    0.051947\n\n# additional train; tfidf ngram1,5; plain logreg\n# 5    0.340676\n# 4    0.319278\n# 3    0.199017\n# 1    0.117034\n# 2    0.023996","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/shopee-sentiment-analysis/test.csv\")\nsubmission.drop('review', axis=1, inplace=True)\nsubmission['rating'] = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.rating.value_counts(normalize=True)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"5    0.328347\n4    0.289275\n3    0.184222\n1    0.143131\n2    0.055025\nName: rating, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vanila tfidf & logreg in subset training -> 0.41\n# 5    0.346832\n# 4    0.337713\n# 3    0.198074\n# 1    0.083721\n# 2    0.033660\n\n# two steps prediction -> 0.40\n# 5    0.350506\n# 4    0.328032\n# 3    0.204975\n# 1    0.081884\n# 2    0.034604\n\n# fasttext tuned\n# 4    0.419696\n# 3    0.263690\n# 5    0.185166\n# 1    0.079501\n# 2    0.051947","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}