{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Cat\n",
    "Problem statement: Predict Cat given features with a lot categorical types\n",
    "\n",
    "Type: Binary Classification\n",
    "\n",
    "Performance metric: Area Under the ROC Curve (AUC score)\n",
    "\n",
    "What I did in this notebook:\n",
    "\n",
    "This is my 2nd notebook on this competiton. My previous notebook is experimenting with several categorical encoding and feature engineering could be found [here](https://www.kaggle.com/ajisamudra/experimenting-with-categorical-encoding).\n",
    "1. Modelling using CatBoostClassifier\n",
    "2. Modelling using Multi Layer Perceptron / Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as cat_encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding for features ord_1, ord_2, ord_3, ord_4\n",
    "\n",
    "def encode_ord_1(x):\n",
    "    if x == \"Novice\":\n",
    "        return 0\n",
    "    elif x == \"Contributor\":\n",
    "        return 1\n",
    "    elif x == \"Expert\":\n",
    "        return 2\n",
    "    elif x == \"Master\":\n",
    "        return 3\n",
    "    elif x == \"Grandmaster\":\n",
    "        return 4\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def encode_ord_2(x):\n",
    "    if x == \"Freezing\":\n",
    "        return 0\n",
    "    elif x == \"Cold\":\n",
    "        return 1\n",
    "    elif x == \"Warm\":\n",
    "        return 2\n",
    "    elif x == \"Hot\":\n",
    "        return 3\n",
    "    elif x == \"Boiling Hot\":\n",
    "        return 4\n",
    "    elif x == \"Lava Hot\":\n",
    "        return 5\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def encode_ord_3(x):\n",
    "    if x == \"a\":\n",
    "        return 0\n",
    "    elif x == \"b\":\n",
    "        return 1\n",
    "    elif x == \"c\":\n",
    "        return 2\n",
    "    elif x == \"d\":\n",
    "        return 3\n",
    "    elif x == \"e\":\n",
    "        return 4\n",
    "    elif x == \"f\":\n",
    "        return 5\n",
    "    elif x == \"g\":\n",
    "        return 6\n",
    "    elif x == \"h\":\n",
    "        return 7\n",
    "    elif x == \"i\":\n",
    "        return 8\n",
    "    elif x == \"j\":\n",
    "        return 9\n",
    "    elif x == \"k\":\n",
    "        return 10\n",
    "    elif x == \"l\":\n",
    "        return 11\n",
    "    elif x == \"m\":\n",
    "        return 12\n",
    "    elif x == \"n\":\n",
    "        return 13\n",
    "    elif x == \"o\":\n",
    "        return 14\n",
    "    elif x == \"p\":\n",
    "        return 15\n",
    "    elif x == \"q\":\n",
    "        return 16\n",
    "    elif x == \"r\":\n",
    "        return 17\n",
    "    elif x == \"s\":\n",
    "        return 18\n",
    "    elif x == \"t\":\n",
    "        return 19\n",
    "    elif x == \"u\":\n",
    "        return 20\n",
    "    elif x == \"v\":\n",
    "        return 21\n",
    "    elif x == \"w\":\n",
    "        return 22\n",
    "    elif x == \"x\":\n",
    "        return 23\n",
    "    elif x == \"y\":\n",
    "        return 24\n",
    "    elif x == \"z\":\n",
    "        return 25\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "# Label encoding for bin_3 and bin_4\n",
    "def encode_bin_3(x):\n",
    "    if x == \"T\":\n",
    "        return 1\n",
    "    elif x == \"F\":\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def encode_bin_4(x):\n",
    "    if x == \"Y\":\n",
    "        return 1\n",
    "    elif x == \"N\":\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Read file\n",
    "train = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/test.csv')\n",
    "\n",
    "# Drop id on train and test dataset\n",
    "train = train.drop('id', axis = 1)\n",
    "test = test.drop('id', axis = 1)\n",
    "\n",
    "# Select only baseline features\n",
    "features = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'day', 'month', 'nom_0',\n",
    "                     'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n",
    "                     'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\n",
    "\n",
    "train_target = train['target']\n",
    "train = train[features]\n",
    "test = test[features]\n",
    "\n",
    "# Replacing nom_6 value 'a885aacec' in test_dataset with 'missing_value' because the value is not seen at training data\n",
    "test.loc[test.nom_6 == \"a885aacec\", 'nom_6'] = \"missing_value\"\n",
    "\n",
    "# Label encode first bin_3 and bin_4\n",
    "train['bin_3'] = train.bin_3.apply(lambda x: encode_bin_3(x))\n",
    "train['bin_4'] = train.bin_4.apply(lambda x: encode_bin_4(x))\n",
    "test['bin_3'] = test.bin_3.apply(lambda x: encode_bin_3(x))\n",
    "test['bin_4'] = test.bin_4.apply(lambda x: encode_bin_4(x))\n",
    "\n",
    "# Impute with constant\n",
    "columns = train.columns\n",
    "\n",
    "for i in columns:\n",
    "    \n",
    "    if train[i].dtype == object:\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy= 'constant', add_indicator= True)\n",
    "    else:\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy= 'constant', fill_value= -1, add_indicator= True)\n",
    "\n",
    "    imputer.fit(train[i].to_numpy().reshape(-1,1))\n",
    "    \n",
    "    train[i] = imputer.transform(train[i].to_numpy().reshape(-1,1))\n",
    "    test[i] = imputer.transform(test[i].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Create interactions\n",
    "train['nom_1_nom_2'] = train.nom_1 + \"_\" + train.nom_2\n",
    "train['nom_1_nom_3'] = train.nom_1 + \"_\" + train.nom_3\n",
    "train['nom_1_nom_4'] = train.nom_1 + \"_\" + train.nom_4\n",
    "train['nom_2_nom_3'] = train.nom_2 + \"_\" + train.nom_3\n",
    "train['nom_2_nom_4'] = train.nom_2 + \"_\" + train.nom_4\n",
    "train['nom_3_nom_4'] = train.nom_3 + \"_\" + train.nom_4\n",
    "test['nom_1_nom_2'] = test.nom_1 + \"_\" + test.nom_2\n",
    "test['nom_1_nom_3'] = test.nom_1 + \"_\" + test.nom_3\n",
    "test['nom_1_nom_4'] = test.nom_1 + \"_\" + test.nom_4\n",
    "test['nom_2_nom_3'] = test.nom_2 + \"_\" + test.nom_3\n",
    "test['nom_2_nom_4'] = test.nom_2 + \"_\" + test.nom_4\n",
    "test['nom_3_nom_4'] = test.nom_3 + \"_\" + test.nom_4\n",
    "\n",
    "# Interaction Exp 3\n",
    "train['bin_all_sum'] = train.bin_0 + train.bin_1 + train.bin_2 + train.bin_3 + train.bin_4\n",
    "train['bin_all_mul'] = train.bin_0 * train.bin_1 * train.bin_2 * train.bin_3 * train.bin_4\n",
    "test['bin_all_sum'] = test.bin_0 + test.bin_1 + test.bin_2 + test.bin_3 + test.bin_4\n",
    "test['bin_all_mul'] = test.bin_0 * test.bin_1 * test.bin_2 * test.bin_3 * test.bin_4\n",
    "\n",
    "# Create cyclical features from day and month\n",
    "train['day_sin7'] = np.sin(2*np.pi*train['day']/7)\n",
    "train['day_sin14'] = np.sin(2*np.pi*train['day']/14)\n",
    "train['day_sin14add'] = np.sin(2*np.pi*train['day']/14)*3.5\n",
    "train['month_sin12'] = np.sin(2*np.pi*train['month']/12)\n",
    "train['month_sin24'] = np.sin(2*np.pi*train['month']/24)\n",
    "train['month_sin24_add'] = np.sin(2*np.pi*train['month']/24)*6\n",
    "test['day_sin7'] = np.sin(2*np.pi*test['day']/7)\n",
    "test['day_sin14'] = np.sin(2*np.pi*test['day']/14)\n",
    "test['day_sin14add'] = np.sin(2*np.pi*test['day']/14)*3.5\n",
    "test['month_sin12'] = np.sin(2*np.pi*test['month']/12)\n",
    "test['month_sin24'] = np.sin(2*np.pi*test['month']/24)\n",
    "test['month_sin24_add'] = np.sin(2*np.pi*test['month']/24)*6\n",
    "\n",
    "# Ordinal Encoding\n",
    "train['ord_1'] = train.ord_1.apply(lambda x: encode_ord_1(x))\n",
    "train['ord_2'] = train.ord_2.apply(lambda x: encode_ord_2(x))\n",
    "train['ord_3'] = train.ord_3.apply(lambda x: encode_ord_3(x))\n",
    "train['ord_4'] = train.ord_4.str.lower().apply(lambda x: encode_ord_3(x))\n",
    "test['ord_1'] = test.ord_1.apply(lambda x: encode_ord_1(x))\n",
    "test['ord_2'] = test.ord_2.apply(lambda x: encode_ord_2(x))\n",
    "test['ord_3'] = test.ord_3.apply(lambda x: encode_ord_3(x))\n",
    "test['ord_4'] = test.ord_4.str.lower().apply(lambda x: encode_ord_3(x))\n",
    "\n",
    "# Ordinal Encoding Square\n",
    "# First normalize with maximum label for faster convergence,\n",
    "# Subtract with 0.5 and square it\n",
    "train['ord_1_sqr_mid'] = ((train.ord_1 / 4) - 0.5)**2\n",
    "train['ord_2_sqr_mid'] = ((train.ord_2 / 5) - 0.5)**2\n",
    "train['ord_3_sqr_mid'] = ((train.ord_3 / 25) - 0.5)**2\n",
    "train['ord_4_sqr_mid'] = ((train.ord_4 / 25) - 0.5)**2\n",
    "test['ord_1_sqr_mid'] = ((test.ord_1 / 4) - 0.5)**2\n",
    "test['ord_2_sqr_mid'] = ((test.ord_2 / 5) - 0.5)**2\n",
    "test['ord_3_sqr_mid'] = ((test.ord_3 / 25) - 0.5)**2\n",
    "test['ord_4_sqr_mid'] = ((test.ord_4 / 25) - 0.5)**2\n",
    "# Square bot not centered\n",
    "train['ord_1_sqr'] = ((train.ord_1 / 4))**2\n",
    "train['ord_2_sqr'] = ((train.ord_2 / 5))**2\n",
    "train['ord_3_sqr'] = ((train.ord_3 / 25))**2\n",
    "train['ord_4_sqr'] = ((train.ord_4 / 25))**2\n",
    "test['ord_1_sqr'] = ((test.ord_1 / 4))**2\n",
    "test['ord_2_sqr'] = ((test.ord_2 / 5))**2\n",
    "test['ord_3_sqr'] = ((test.ord_3 / 25))**2\n",
    "test['ord_4_sqr'] = ((test.ord_4 / 25))**2\n",
    "# Log Transform\n",
    "train['ord_1_log'] = np.log1p((train.ord_1 / 4))\n",
    "train['ord_2_log'] = np.log1p((train.ord_2 / 5))\n",
    "train['ord_3_log'] = np.log1p((train.ord_3 / 25))\n",
    "train['ord_4_log'] = np.log1p((train.ord_4 / 25))\n",
    "test['ord_1_log'] = np.log1p((test.ord_1 / 4))\n",
    "test['ord_2_log'] = np.log1p((test.ord_2 / 5))\n",
    "test['ord_3_log'] = np.log1p((test.ord_3 / 25))\n",
    "test['ord_4_log'] = np.log1p((test.ord_4 / 25))\n",
    "\n",
    "# Day & Month centered square\n",
    "train['day_sqr_mid'] = ((train.day / 7) - 0.5)**2\n",
    "train['month_sqr_mid'] = ((train.month / 12) - 0.5)**2\n",
    "test['day_sqr_mid'] = ((test.day / 7) - 0.5)**2\n",
    "test['month_sqr_mid'] = ((test.month / 12) - 0.5)**2\n",
    "\n",
    "# Day & Month not-centered square\n",
    "train['day_sqr_mid'] = ((train.day / 7))**2\n",
    "train['month_sqr_mid'] = ((train.month / 12))**2\n",
    "test['day_sqr_mid'] = ((test.day / 7))**2\n",
    "test['month_sqr_mid'] = ((test.month / 12))**2\n",
    "\n",
    "# Day & Month Log transform\n",
    "train['day_log'] = np.log1p((train.day / 7))\n",
    "train['month_log'] = np.log1p((train.month / 12))\n",
    "test['day_log'] = np.log1p((test.day / 7))\n",
    "test['month_log'] = np.log1p((test.month / 12))\n",
    "\n",
    "# Ord 1-4 * day and month\n",
    "train['month_ord_1'] = train.ord_1 * train.month\n",
    "train['month_ord_2'] = train.ord_2 * train.month\n",
    "train['month_ord_3'] = train.ord_3 * train.month\n",
    "train['month_ord_4'] = train.ord_4 * train.month\n",
    "test['month_ord_1'] = test.ord_1 * test.month\n",
    "test['month_ord_2'] = test.ord_2 * test.month\n",
    "test['month_ord_3'] = test.ord_3 * test.month\n",
    "test['month_ord_4'] = test.ord_4 * test.month\n",
    "train['day_ord_1'] = train.ord_1 * train.day\n",
    "train['day_ord_2'] = train.ord_2 * train.day\n",
    "train['day_ord_3'] = train.ord_3 * train.day\n",
    "train['day_ord_4'] = train.ord_4 * train.day\n",
    "test['day_ord_1'] = test.ord_1 * test.day\n",
    "test['day_ord_2'] = test.ord_2 * test.day\n",
    "test['day_ord_3'] = test.ord_3 * test.day\n",
    "test['day_ord_4'] = test.ord_4 * test.day\n",
    "\n",
    "# Centered Squared\n",
    "train['month_ord_1_sqr_mid'] = ((train.month_ord_1 / train.month_ord_1.max() ) - 0.5)**2\n",
    "train['month_ord_2_sqr_mid'] = ((train.month_ord_2 / train.month_ord_2.max() ) - 0.5)**2\n",
    "train['month_ord_3_sqr_mid'] = ((train.month_ord_3 / train.month_ord_3.max() ) - 0.5)**2\n",
    "train['month_ord_4_sqr_mid'] = ((train.month_ord_4 / train.month_ord_4.max() ) - 0.5)**2\n",
    "test['month_ord_1_sqr_mid'] = ((test.month_ord_1 / train.month_ord_1.max()) - 0.5)**2\n",
    "test['month_ord_2_sqr_mid'] = ((test.month_ord_2 / train.month_ord_2.max()) - 0.5)**2\n",
    "test['month_ord_3_sqr_mid'] = ((test.month_ord_3 / train.month_ord_3.max()) - 0.5)**2\n",
    "test['month_ord_4_sqr_mid'] = ((test.month_ord_4 / train.month_ord_4.max()) - 0.5)**2\n",
    "\n",
    "# Not Centered Squared\n",
    "train['month_ord_1_sqr'] = ((train.month_ord_1 / train.month_ord_1.max() ) )**2\n",
    "train['month_ord_2_sqr'] = ((train.month_ord_2 / train.month_ord_2.max() ) )**2\n",
    "train['month_ord_3_sqr'] = ((train.month_ord_3 / train.month_ord_3.max() ) )**2\n",
    "train['month_ord_4_sqr'] = ((train.month_ord_4 / train.month_ord_4.max() ) )**2\n",
    "test['month_ord_1_sqr'] = ((test.month_ord_1 / train.month_ord_1.max()) )**2\n",
    "test['month_ord_2_sqr'] = ((test.month_ord_2 / train.month_ord_2.max()) )**2\n",
    "test['month_ord_3_sqr'] = ((test.month_ord_3 / train.month_ord_3.max()) )**2\n",
    "test['month_ord_4_sqr'] = ((test.month_ord_4 / train.month_ord_4.max()) )**2\n",
    "\n",
    "# Log\n",
    "train['month_ord_1_log'] = np.log1p(train.month_ord_1 / train.month_ord_1.max())\n",
    "train['month_ord_2_log'] = np.log1p(train.month_ord_2 / train.month_ord_2.max())\n",
    "train['month_ord_3_log'] = np.log1p(train.month_ord_3 / train.month_ord_3.max())\n",
    "train['month_ord_4_log'] = np.log1p(train.month_ord_4 / train.month_ord_4.max())\n",
    "test['month_ord_1_log'] = np.log1p(test.month_ord_1 / train.month_ord_1.max())\n",
    "test['month_ord_2_log'] = np.log1p(test.month_ord_2 / train.month_ord_2.max())\n",
    "test['month_ord_3_log'] = np.log1p(test.month_ord_3 / train.month_ord_3.max())\n",
    "test['month_ord_4_log'] = np.log1p(test.month_ord_4 / train.month_ord_4.max())\n",
    "\n",
    "# Centered Squared\n",
    "train['day_ord_1_sqr_mid'] = ((train.day_ord_1 / train.day_ord_1.max() ) - 0.5)**2\n",
    "train['day_ord_2_sqr_mid'] = ((train.day_ord_2 / train.day_ord_2.max() ) - 0.5)**2\n",
    "train['day_ord_3_sqr_mid'] = ((train.day_ord_3 / train.day_ord_3.max() ) - 0.5)**2\n",
    "train['day_ord_4_sqr_mid'] = ((train.day_ord_4 / train.day_ord_4.max() ) - 0.5)**2\n",
    "test['day_ord_1_sqr_mid'] = ((test.day_ord_1 / train.day_ord_1.max()) - 0.5)**2\n",
    "test['day_ord_2_sqr_mid'] = ((test.day_ord_2 / train.day_ord_2.max()) - 0.5)**2\n",
    "test['day_ord_3_sqr_mid'] = ((test.day_ord_3 / train.day_ord_3.max()) - 0.5)**2\n",
    "test['day_ord_4_sqr_mid'] = ((test.day_ord_4 / train.day_ord_4.max()) - 0.5)**2\n",
    "\n",
    "# Not Centered Squared\n",
    "train['day_ord_1_sqr'] = ((train.day_ord_1 / train.day_ord_1.max() ) )**2\n",
    "train['day_ord_2_sqr'] = ((train.day_ord_2 / train.day_ord_2.max() ) )**2\n",
    "train['day_ord_3_sqr'] = ((train.day_ord_3 / train.day_ord_3.max() ) )**2\n",
    "train['day_ord_4_sqr'] = ((train.day_ord_4 / train.day_ord_4.max() ) )**2\n",
    "test['day_ord_1_sqr'] = ((test.day_ord_1 / train.day_ord_1.max()) )**2\n",
    "test['day_ord_2_sqr'] = ((test.day_ord_2 / train.day_ord_2.max()) )**2\n",
    "test['day_ord_3_sqr'] = ((test.day_ord_3 / train.day_ord_3.max()) )**2\n",
    "test['day_ord_4_sqr'] = ((test.day_ord_4 / train.day_ord_4.max()) )**2\n",
    "\n",
    "# Log\n",
    "train['day_ord_1_log'] = np.log1p(train.day_ord_1 / train.day_ord_1.max())\n",
    "train['day_ord_2_log'] = np.log1p(train.day_ord_2 / train.day_ord_2.max())\n",
    "train['day_ord_3_log'] = np.log1p(train.day_ord_3 / train.day_ord_3.max())\n",
    "train['day_ord_4_log'] = np.log1p(train.day_ord_4 / train.day_ord_4.max())\n",
    "test['day_ord_1_log'] = np.log1p(test.day_ord_1 / train.day_ord_1.max())\n",
    "test['day_ord_2_log'] = np.log1p(test.day_ord_2 / train.day_ord_2.max())\n",
    "test['day_ord_3_log'] = np.log1p(test.day_ord_3 / train.day_ord_3.max())\n",
    "test['day_ord_4_log'] = np.log1p(test.day_ord_4 / train.day_ord_4.max())\n",
    "\n",
    "\n",
    "# Update columns\n",
    "columns = train.columns\n",
    "\n",
    "# Target Encoding to only object features (using target)\n",
    "for i in columns:\n",
    "    if train[i].dtype == object:\n",
    "        target_encoder = cat_encoder.TargetEncoder(smoothing = 0.1)\n",
    "        target_encoder.fit(train[i], train_target)\n",
    "        train[i+\"_target\"] = target_encoder.transform(train[i])\n",
    "        test[i+\"_target\"] = target_encoder.transform(test[i])\n",
    "\n",
    "# Target encoder for day and month\n",
    "target_encoder = cat_encoder.TargetEncoder(smoothing = 0.1)\n",
    "target_encoder.fit(train[['day']], train_target)\n",
    "train['day_target'] = target_encoder.transform(train['day'])\n",
    "test['day_target'] = target_encoder.transform(test['day'])\n",
    "\n",
    "target_encoder = cat_encoder.TargetEncoder(smoothing = 0.1)\n",
    "target_encoder.fit(train[['month']], train_target)\n",
    "train['month_target'] = target_encoder.transform(train['month'])\n",
    "test['month_target'] = target_encoder.transform(test['month'])\n",
    "\n",
    "# Update columns\n",
    "columns = train.columns\n",
    "\n",
    "# Label Encoding to only object features\n",
    "for i in columns:\n",
    "    if train[i].dtype == object:        \n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(train[i])\n",
    "        train[i] = label_encoder.transform(train[i])\n",
    "        test[i] = label_encoder.transform(test[i])\n",
    "\n",
    "# Create ordinal square after label encoding - centered\n",
    "train['ord_5_sqr_mid'] = ((train.ord_5 / train.ord_5.max()) - 0.5)**2\n",
    "test['ord_5_sqr_mid'] = ((test.ord_5 / train.ord_5.max()) - 0.5)**2\n",
    "# Create ordinal square after label encoding - not centered\n",
    "train['ord_5_sqr'] = ((train.ord_5 / train.ord_5.max()) )**2\n",
    "test['ord_5_sqr'] = ((test.ord_5 / train.ord_5.max()) )**2\n",
    "# Log transform\n",
    "train['ord_5_log'] = np.log1p((train.ord_5 / train.ord_5.max()))\n",
    "test['ord_5_log'] = np.log1p((test.ord_5 / train.ord_5.max()))\n",
    "\n",
    "# Ord_5 * day & month\n",
    "train['month_ord_5'] = train.ord_5 * train.month\n",
    "train['day_ord_5'] = train.ord_5 * train.day\n",
    "test['month_ord_5'] = test.ord_5 * test.month\n",
    "test['day_ord_5'] = test.ord_5 * test.day\n",
    "\n",
    "# Centered squared\n",
    "train['month_ord_5_sqr_mid'] = ((train.month_ord_5 / train.month_ord_5.max()) - 0.5)**2\n",
    "test['month_ord_5_sqr_mid'] = ((test.month_ord_5 / train.month_ord_5.max()) - 0.5)**2\n",
    "train['day_ord_5_sqr_mid'] = ((train.day_ord_5 / train.day_ord_5.max()) - 0.5)**2\n",
    "test['day_ord_5_sqr_mid'] = ((test.day_ord_5 / train.day_ord_5.max()) - 0.5)**2\n",
    "\n",
    "# Not centered squared\n",
    "train['month_ord_5_sqr'] = ((train.month_ord_5 / train.month_ord_5.max()) )**2\n",
    "test['month_ord_5_sqr'] = ((test.month_ord_5 / train.month_ord_5.max()) )**2\n",
    "train['day_ord_5_sqr'] = ((train.day_ord_5 / train.day_ord_5.max()) )**2\n",
    "test['day_ord_5_sqr'] = ((test.day_ord_5 / train.day_ord_5.max()) )**2\n",
    "\n",
    "# Log\n",
    "train['month_ord_5_log'] = np.log1p((train.month_ord_5 / train.month_ord_5.max()))\n",
    "test['month_ord_5_log'] = np.log1p((test.month_ord_5 / train.month_ord_5.max()))\n",
    "train['day_ord_5_log'] = np.log1p((train.day_ord_5 / train.day_ord_5.max()))\n",
    "test['day_ord_5_log'] = np.log1p((test.day_ord_5 / train.day_ord_5.max()))\n",
    "\n",
    "\n",
    "# nom9, nom_8, nom_7, nom_6, nom_5, nom_4, nom_3, nom_2, nom_1 * day & month\n",
    "noms = ['nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n",
    "for i in noms:\n",
    "    train[\"month_\"+i] = train[i] * train.month\n",
    "    train[\"day_\"+i] = train[i] * train.day\n",
    "    test[\"month_\"+i] = test[i] * train.month\n",
    "    test[\"day_\"+i] = test[i] * test.day\n",
    "\n",
    "def centered_square(x, xmax):\n",
    "    return ( (x / xmax)-0.5 )**2\n",
    "\n",
    "def square(x, xmax):\n",
    "    return ( (x / xmax))**2\n",
    "\n",
    "def transform_log(x, xmax):\n",
    "    return np.log1p((x / xmax))\n",
    "\n",
    "month_noms = ['month_nom_1', 'month_nom_2', 'month_nom_3', 'month_nom_4', 'month_nom_5', 'month_nom_6', 'month_nom_7', 'month_nom_8', 'month_nom_9']\n",
    "day_noms = ['day_nom_1', 'day_nom_2', 'day_nom_3', 'day_nom_4', 'day_nom_5', 'day_nom_6', 'day_nom_7', 'day_nom_8', 'day_nom_9']\n",
    "\n",
    "# Centered Square\n",
    "for i in month_noms:\n",
    "    train[i+\"_sqr_mid\"] = centered_square( train[i], train[i].max())\n",
    "    test[i+\"_sqr_mid\"] = centered_square( test[i], train[i].max())\n",
    "\n",
    "for i in day_noms:\n",
    "    train[i+\"_sqr_mid\"] = centered_square( train[i], train[i].max())\n",
    "    test[i+\"_sqr_mid\"] = centered_square( test[i], train[i].max())\n",
    "    \n",
    "# Square\n",
    "for i in month_noms:\n",
    "    train[i+\"_sqr\"] = square( train[i], train[i].max())\n",
    "    test[i+\"_sqr\"] = square( test[i], train[i].max())\n",
    "\n",
    "for i in day_noms:\n",
    "    train[i+\"_sqr\"] = square( train[i], train[i].max())\n",
    "    test[i+\"_sqr\"] = square( test[i], train[i].max())\n",
    "\n",
    "# Log\n",
    "for i in month_noms:\n",
    "    train[i+\"_log\"] = transform_log( train[i], train[i].max())\n",
    "    test[i+\"_log\"] = transform_log( test[i], train[i].max())\n",
    "\n",
    "for i in day_noms:\n",
    "    train[i+\"_log\"] = transform_log( train[i], train[i].max())\n",
    "    test[i+\"_log\"] = transform_log( test[i], train[i].max())\n",
    "\n",
    "# bin_0, bin_1, bin_2, bin_3, bin_4 * day & month\n",
    "bins = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n",
    "for i in bins:\n",
    "    train[\"month_\"+i] = train[i] * train.month\n",
    "    train[\"day_\"+i] = train[i] * train.day\n",
    "    test[\"month_\"+i] = test[i] * train.month\n",
    "    test[\"day_\"+i] = test[i] * test.day\n",
    "\n",
    "month_bins = ['month_bin_0', 'month_bin_1', 'month_bin_2', 'month_bin_3', 'month_bin_4']\n",
    "day_bins = ['day_bin_0', 'day_bin_1', 'day_bin_2', 'day_bin_3', 'day_bin_4']\n",
    "\n",
    "# Centered Square\n",
    "for i in month_bins:\n",
    "    train[i+\"_sqr_mid\"] = centered_square( train[i], train[i].max())\n",
    "    test[i+\"_sqr_mid\"] = centered_square( test[i], train[i].max())\n",
    "\n",
    "for i in day_bins:\n",
    "    train[i+\"_sqr_mid\"] = centered_square( train[i], train[i].max())\n",
    "    test[i+\"_sqr_mid\"] = centered_square( test[i], train[i].max())\n",
    "    \n",
    "# Square\n",
    "for i in month_bins:\n",
    "    train[i+\"_sqr\"] = square( train[i], train[i].max())\n",
    "    test[i+\"_sqr\"] = square( test[i], train[i].max())\n",
    "\n",
    "for i in day_bins:\n",
    "    train[i+\"_sqr\"] = square( train[i], train[i].max())\n",
    "    test[i+\"_sqr\"] = square( test[i], train[i].max())\n",
    "\n",
    "# Doing decomposition only to 68 features from earlier\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(train.iloc[:, 0:67])\n",
    "decom_X_train = pca.transform(train.iloc[:, 0:67])\n",
    "decom_X_test = pca.transform(test.iloc[:,0:67])\n",
    "\n",
    "# Clustering and do target encoder on the target\n",
    "kmeans = KMeans(n_clusters = 2, random_state = 41)\n",
    "kmeans.fit(decom_X_train)\n",
    "train_cluster = kmeans.predict(decom_X_train)\n",
    "test_cluster = kmeans.predict(decom_X_test)\n",
    "\n",
    "train = pd.concat([ train,  pd.DataFrame(train_cluster, columns = ['cluster']) ], axis = 1)\n",
    "test = pd.concat([ test,  pd.DataFrame(test_cluster, columns = ['cluster'])], axis = 1)\n",
    "\n",
    "# Target encoder for cluster\n",
    "target_encoder = cat_encoder.TargetEncoder(smoothing = 0.1)\n",
    "target_encoder.fit(train[['cluster']], train_target)\n",
    "train['cluster_target'] = target_encoder.transform(train['cluster'])\n",
    "test['cluster_target'] = target_encoder.transform(test['cluster'])\n",
    "\n",
    "# Update columns\n",
    "columns = train.columns\n",
    "\n",
    "# Standardize the values\n",
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before experimenting with undersampling and LGBM, we need to split training for validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train,\n",
    "#                                                   train_target,\n",
    "#                                                   test_size = 0.2,\n",
    "#                                                   stratify = train_target,\n",
    "#                                                   random_state = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_auc(model, X_train, X_val, y_train, y_val):\n",
    "#     model.fit(X=X_train,y =y_train)\n",
    "#     y_pred = model.predict_proba(X_val)\n",
    "    \n",
    "#     score = roc_auc_score(y_val, y_pred[:,1])\n",
    "#     print(\"AUC Score on Validation: {}\".format(score))\n",
    "    \n",
    "# # tuned_lgbm = LGBMClassifier(n_estimators = 300, lambda_l1 = 0.5, learning_rate = 0.1, num_leaves = 6, max_depth = 5,random_state = 41)\n",
    "# # evaluate_auc(tuned_lgbm, X_train, X_val, y_train, y_val)\n",
    "# # AUC Score on Validation: 0.7955373141802544 -> all features 217\n",
    "# # AUC Score on Validation: 0.7955373141802544 -> Adding cluster target\n",
    "# # AUC Score on Validation: 0.7957226405432072 -> selectfrommodel\n",
    "# # AUC Score on Validation: 0.7956081111848645 -> applying lambda_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_logreg = LogisticRegression(random_state = 41)\n",
    "# evaluate_auc(plain_logreg, X_train_new, X_val_new, y_train, y_val)\n",
    "# # AUC Score on Validation: 0.7909603468202064 -> all features 217\n",
    "# # AUC Score on Validation: 0.7909445152179959 Adding cluster target \n",
    "# # AUC Score on Validation: 0.786667054899717 -> selectfrommodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation Matrix\n",
    "# cor_mtx = pd.concat([ pd.DataFrame(train, columns = columns), train_target], axis = 1).corr()\n",
    "# print(\"Pearson correlation coefficient:\")\n",
    "\n",
    "# cor_mtx[ (cor_mtx.target >= 0.08) | (cor_mtx.target <= -0.08)]['target'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA for dimensional reduction\n",
    "\n",
    "# # Lets build PCA that will retain 95% variance on the data\n",
    "# pca = PCA(n_components=0.99)\n",
    "# pca.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decom_train = pca.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plotting the Cumulative Summation of the Explained Variance\n",
    "# plt.figure()\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Variance (%)') #for each component\n",
    "# plt.title('Data Explained Variance')\n",
    "# plt.axhline(y=0.95, color = 'r', linestyle='--')\n",
    "# plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize = 12)\n",
    "# plt.show()\n",
    "\n",
    "# # Based on the picture, after 70 components there are no significant increment of vairance % of total data\n",
    "# # I decide to only use 70 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# # Tuned Logistic Regression\n",
    "# # Training model\n",
    "# logit = LogisticRegression(C=0.05, solver=\"lbfgs\", max_iter=5000)\n",
    "# logit.fit(train, train_target)\n",
    "\n",
    "# # Predict\n",
    "# y_pred = logit.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make file for submission\n",
    "# exp6_submission = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/sample_submission.csv')\n",
    "# exp6_submission['target'] = y_pred[:,1]\n",
    "# exp6_submission.to_csv('tuned_logreg_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See whether we have cluster or not in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define RandomUnderSampler\n",
    "# under_sampler = RandomUnderSampler(sampling_strategy = 1, random_state = 41)\n",
    "# X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_lgbm = LGBMClassifier(n_estimators = 300, learning_rate = 0.1, num_leaves = 6, max_depth = 5,random_state = 41)\n",
    "# evaluate_auc(tuned_lgbm, X_train_resampled, X_val, y_train_resampled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Doing decomposition only to 68 features from earlier\n",
    "# pca = PCA(n_components = 2)\n",
    "# pca.fit(X_train_resampled[:, 0:67])\n",
    "# decom_X_train = pca.transform(X_train_resampled[:, 0:67])\n",
    "# decom_X_val = pca.transform(X_val[:,0:67])\n",
    "# decom_X_train.shape , decom_X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot scatter on reduced dimensionality\n",
    "# decom_data = pd.concat( [pd.DataFrame(decom_X) , y_train_resampled], axis = 1 )\n",
    "# sns.pairplot(data= decom_data, hue = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters = 2, random_state = 41)\n",
    "# kmeans.fit(decom_X_train)\n",
    "# train_cluster = kmeans.predict(decom_X_train)\n",
    "# val_cluster = kmeans.predict(decom_X_val)\n",
    "\n",
    "# X_train_resampled = pd.concat([ pd.DataFrame(X_train_resampled, columns = columns), pd.DataFrame(train_cluster, columns = ['cluster']) ], axis = 1)\n",
    "# X_val = pd.concat([ pd.DataFrame(X_val, columns = columns), pd.DataFrame(val_cluster, columns = ['cluster'])], axis = 1)\n",
    "\n",
    "# # Target encoder for day and month\n",
    "# target_encoder = cat_encoder.TargetEncoder(smoothing = 0.1)\n",
    "# target_encoder.fit(X_train_resampled[['cluster']], y_train_resampled)\n",
    "# X_train_resampled['cluster_target'] = target_encoder.transform( (X_train_resampled['cluster']).astype(int) )\n",
    "# X_val['cluster_target'] = target_encoder.transform((X_val['cluster']).astype(int))\n",
    "\n",
    "# columns = X_train_resampled.columns\n",
    "\n",
    "# # Standardize the values\n",
    "# scaler = StandardScaler()\n",
    "# X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "# X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate with additional cluster information\n",
    "# tuned_lgbm = LGBMClassifier(n_estimators = 300, learning_rate = 0.1, num_leaves = 6, max_depth = 5,random_state = 41)\n",
    "# evaluate_auc(tuned_lgbm, X_train_resampled, X_val, y_train_resampled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot scatter on reduced dimensionality\n",
    "# decom_data = pd.concat( [pd.DataFrame(decom_X_train, columns = ['pca_1', 'pca_2']) , y_train_resampled, pd.Series(train_cluster)], axis = 1 )\n",
    "# sns.pairplot(data= decom_data, hue = 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of positive labels on training data\n",
    "# train_target.value_counts()\n",
    "\n",
    "# # The idea of undersampling is we will eliminate the majority class which is the negative labels\n",
    "# # So it will have the same number of samples with positive label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define RandomUnderSampler\n",
    "# under_sampler = RandomUnderSampler(sampling_strategy = 1, random_state = 41)\n",
    "# X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How will the score if we only train using resamples training data\n",
    "# evaluate_auc(logit, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "# Wow it's quite the same (0.7877)\n",
    "# We will use under-sampling strategy to fasten our experimentation to find the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plain LGBM Classifier\n",
    "# plain_lgbm = LGBMClassifier(random_state = 41)\n",
    "# evaluate_auc(plain_lgbm, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "\n",
    "# # With only plain LGBM we could improve auc validation from 0.7877 to 0.7926\n",
    "# # After adding to ~ 200 features auc 0.7927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets try to find best hyperparameter for LGBM\n",
    "# # These are three important hyperparameter for structure the model (Phase 1 hyperparamter tuning)\n",
    "\n",
    "# param_grid = {'num_leaves' : [16],\n",
    "#               'max_depth': [6],\n",
    "#               'lambda_l1': [0.005, 0.01, 2]}\n",
    "\n",
    "# scv = StratifiedKFold(n_splits = 3, random_state = 41)\n",
    "\n",
    "# grid_searcher = GridSearchCV(estimator = plain_lgbm, scoring = \"roc_auc\" , param_grid = param_grid, cv = scv, verbose=1, n_jobs = -1 )\n",
    "# grid_searcher.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_searcher.best_params_, grid_searcher.best_score_\n",
    "# # 200 features\n",
    "# # ({'max_depth': 3, 'n_estimators': 400, 'num_leaves': 5}, 0.7951495088651278)\n",
    "\n",
    "# # After adding target clusters\n",
    "# # ({'lambda_l1': 0.01, 'max_depth': 4, 'num_leaves': 7}, 0.7873061596306101)\n",
    "# # ({'lambda_l1': 0.01, 'max_depth': 6, 'num_leaves': 10}, 0.7895831487743887)\n",
    "# # ({'lambda_l1': 0.01, 'max_depth': 6, 'num_leaves': 16}, 0.7905603566960145)\n",
    "# # ({'lambda_l1': 0.005, 'max_depth': 6, 'num_leaves': 16}, 0.7905883541727441)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets try to find best hyperparameter for LGBM\n",
    "# # These are three important hyperparameter for convergence (Phase 2 hyperparamter tuning)\n",
    "\n",
    "# tuned_lgbm = LGBMClassifier(num_leaves = 16, max_depth = 6, lambda_l1 = 0.005, random_state = 41)\n",
    "\n",
    "# param_grid = {'learning_rate' : [0.11, 0.2, 0.3]}\n",
    "\n",
    "# scv = StratifiedKFold(n_splits = 3, random_state = 41)\n",
    "\n",
    "# grid_searcher = GridSearchCV(estimator = tuned_lgbm, scoring = \"roc_auc\" , param_grid = param_grid, cv = scv, verbose=1, n_jobs = -1 )\n",
    "# grid_searcher.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_searcher.best_params_, grid_searcher.best_score_\n",
    "# # ({'learning_rate': 0.1778279410038923}, 0.794386184358966)\n",
    "# # ({'learning_rate': 0.1}, 0.7951577528090543)\n",
    "# # Add Features 200\n",
    "# # ({'learning_rate': 0.03162277660168379}, 0.7883761918789878)\n",
    "# # ({'learning_rate': 0.1}, 0.7951495088651278)\n",
    "# # ({'learning_rate': 0.11}, 0.7915211779314236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGBM with best hyperparameter\n",
    "# tuned_lgbm = LGBMClassifier(n_estimators = 300, learning_rate = 0.1, num_leaves = 6, max_depth = 5,random_state = 41)\n",
    "\n",
    "# print(\"Training on resampled training instances\")\n",
    "# # Training using resampled instances\n",
    "# evaluate_auc(tuned_lgbm, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "# print(\"Training on all training instances\")\n",
    "# # Training using all itraining instances\n",
    "# evaluate_auc(tuned_lgbm, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# # It validates the grid search result\n",
    "# # The validation score is quite the same ~ 0.796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tried generate several prediction using several under sampling\n",
    "\n",
    "# best_lgbm = LGBMClassifier(n_estimators = 300, learning_rate = 0.1, num_leaves = 6, max_depth = 5,random_state = 41)\n",
    "\n",
    "# def predict_val_random_sample(model, i, X_train, X_val, y_train, y_val):\n",
    "#     # Define RandomUnderSampler\n",
    "#     print(\"Random State: {}\".format(i))\n",
    "#     under_sampler = RandomUnderSampler(sampling_strategy = 1, random_state = i)\n",
    "#     X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "#     # Evaluate AUC on validation dataset\n",
    "#     evaluate_auc(model, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "#     # Training model and predict\n",
    "#     model.fit(X_train_resampled, y_train_resampled)\n",
    "#     # Predict\n",
    "#     y_pred = model.predict_proba(X_val)\n",
    "#     return y_pred[:,1]\n",
    "\n",
    "# # Initiate list for result several LGBM prediction\n",
    "# d1 = pd.DataFrame()\n",
    "# d2 = pd.DataFrame()\n",
    "# d3 = pd.DataFrame()\n",
    "# d4 = pd.DataFrame()\n",
    "# dfs = [d1, d2, d3, d4]\n",
    "\n",
    "# # Initiate several random state\n",
    "# rands = [1, 30, 41, 70]\n",
    "\n",
    "# for i in range(4):\n",
    "#     dfs[i] = predict_val_random_sample(best_lgbm, rands[i], X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See difference score on validation score by several model which are built on different dataset\n",
    "# for i in dfs:    \n",
    "#     score = roc_auc_score(y_val,i)\n",
    "#     print(\"AUC Score on Validation: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensemble the prediction with rank-based weighted averaging\n",
    "\n",
    "# # Rank first all the predictions\n",
    "# rank_1 = scipy.stats.rankdata(dfs[0], method = 'dense')\n",
    "# rank_2 = scipy.stats.rankdata(dfs[1], method = 'dense')\n",
    "# rank_3 = scipy.stats.rankdata(dfs[2], method = 'dense')\n",
    "# rank_4 = scipy.stats.rankdata(dfs[3], method = 'dense')\n",
    "\n",
    "# # Average the rank\n",
    "# avg_rank = (0.2 * rank_1 + 0.2 * rank_2 + 0.3 * rank_3 + 0.3 * rank_4)\n",
    "\n",
    "# # Scale the average rank to 0-1\n",
    "# final_result = (avg_rank - avg_rank.min()) / (avg_rank.max() - avg_rank.min())\n",
    "\n",
    "# score = roc_auc_score(y_val,final_result)\n",
    "# print(\"AUC Score on Validation: {}\".format(score))\n",
    "# The result better than all 4 LGBM model. the score on validation is 0.79699"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_1[1], rank_2[1], rank_3[1], rank_4[1], avg_rank[1], final_result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Ensemble LGBM on all training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGBM with best hyperparameter\n",
    "# tuned_lgbm = LGBMClassifier(n_estimators = 300, learning_rate = 0.1, num_leaves = 6, max_depth = 5,random_state = 41)\n",
    "\n",
    "# # Training on all dataset and predict test\n",
    "# def predict_test_random_sample(model, i, X_train, X_test, y_train):\n",
    "#     # Define RandomUnderSampler\n",
    "#     print(\"Random State: {}\".format(i))\n",
    "#     under_sampler = RandomUnderSampler(sampling_strategy = 1, random_state = i)\n",
    "#     X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "#     print(\"Start Training\")\n",
    "#     # Training model and predict\n",
    "#     model.fit(X_train_resampled, y_train_resampled)\n",
    "#     # Predict\n",
    "#     print(\"Start Predicting\")\n",
    "#     y_pred = model.predict_proba(X_test)\n",
    "#     return y_pred[:,1]\n",
    "\n",
    "# # Initiate list for result several LGBM prediction\n",
    "# d1 = pd.DataFrame()\n",
    "# d2 = pd.DataFrame()\n",
    "# d3 = pd.DataFrame()\n",
    "# d4 = pd.DataFrame()\n",
    "# dfs = [d1, d2, d3, d4]\n",
    "\n",
    "# # Initiate several random state\n",
    "# rands = [1, 30, 41, 70]\n",
    "\n",
    "# # Training on all training dataset\n",
    "# for i in range(4):\n",
    "#     dfs[i] = predict_test_random_sample(tuned_lgbm, rands[i], train, test, train_target)\n",
    "\n",
    "# # Ensemble the prediction with rank-based weighted averaging\n",
    "# # Rank first all the predictions\n",
    "# rank_1 = scipy.stats.rankdata(dfs[0], method = 'dense')\n",
    "# rank_2 = scipy.stats.rankdata(dfs[1], method = 'dense')\n",
    "# rank_3 = scipy.stats.rankdata(dfs[2], method = 'dense')\n",
    "# rank_4 = scipy.stats.rankdata(dfs[3], method = 'dense')\n",
    "\n",
    "# # Average the rank\n",
    "# avg_rank = (0.2 * rank_1 + 0.2 * rank_2 + 0.3 * rank_3 + 0.3 * rank_4)\n",
    "\n",
    "# # Scale the average rank to 0-1\n",
    "# final_result = (avg_rank - avg_rank.min()) / (avg_rank.max() - avg_rank.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make file for submission\n",
    "# exp8_submission = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/sample_submission.csv')\n",
    "# exp8_submission['target'] = final_result\n",
    "# exp8_submission.to_csv('ensemble_lgbm_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See difference Logistic Regression performance on sampled and all training dataset\n",
    "\n",
    "# plain_logreg = LogisticRegression(random_state = 41)\n",
    "# print(\"Sampled training\")\n",
    "# evaluate_auc(plain_logreg, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "# print(\"All training\")\n",
    "# evaluate_auc(plain_logreg, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# # The result is quite the same, we will use the sampled dataset for faster iteration on hypeparameter tuning\n",
    "# # Plain LogReg AUC 0.7881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets try to find best hyperparameter for LogisticRegression\n",
    "# # These are three important hyperparameter\n",
    "# plain_logreg = LogisticRegression(random_state = 41)\n",
    "\n",
    "# param_grid = {'C' : [50, 55, 60],\n",
    "#               'solver': [\"newton-cg\"],\n",
    "#               'max_iter': [50]}\n",
    "\n",
    "# scv = StratifiedKFold(n_splits = 3, random_state = 41)\n",
    "\n",
    "# grid_searcher = GridSearchCV(estimator = plain_logreg, scoring = \"roc_auc\" , param_grid = param_grid, cv = scv, verbose=1, n_jobs = -1 )\n",
    "# grid_searcher.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_searcher.best_params_ , grid_searcher.best_score_\n",
    "# # ({'C': 0.8, 'max_iter': 50, 'solver': 'newton-cg'}, 0.7874683296881111)\n",
    "# # ({'C': 0.9, 'max_iter': 50, 'solver': 'newton-cg'}, 0.7875001099038554)\n",
    "# # ({'C': 1.1, 'max_iter': 50, 'solver': 'newton-cg'}, 0.7875542822254484)\n",
    "# # ({'C': 1.5, 'max_iter': 50, 'solver': 'newton-cg'}, 0.7876348764010777)\n",
    "# # ({'C': 3, 'max_iter': 50, 'solver': 'newton-cg'}, 0.7877894925255164)\n",
    "# # ({'C': 50, 'max_iter': 50, 'solver': 'newton-cg'}, 0.7879384137628845)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See the performance on tuned LogReg\n",
    "# tuned_logreg = LogisticRegression(C = 50, max_iter = 50, solver = 'newton-cg', random_state = 41)\n",
    "\n",
    "# print(\"Sampled training\")\n",
    "# evaluate_auc(tuned_logreg, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "# print(\"All training\")\n",
    "# evaluate_auc(tuned_logreg, X_train, X_val, y_train, y_val)\n",
    "# # Tuned logreg have AUC 0.7887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See difference CatBoost performance on sampled and all training datasets\n",
    "# plain_catb = CatBoostClassifier(iterations = 50, loss_function = \"Logloss\", eval_metric = \"AUC\", verbose = False)\n",
    "# print(\"Sampled training\")\n",
    "# evaluate_auc(plain_catb, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "# print(\"All training\")\n",
    "# evaluate_auc(plain_catb, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# # Plain CatBoost performance 0.795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets try to find best hyperparameter for CatBoost\n",
    "# # These are three important hyperparameter\n",
    "# plain_catb = CatBoostClassifier(iterations = 50, loss_function = \"Logloss\", eval_metric = \"AUC\", verbose = False)\n",
    "\n",
    "# param_grid = {'depth' : [8, 13, 15],\n",
    "#               'l2_leaf_reg': [1, 1.2, 1.5],\n",
    "#               'learning_rate': [0.03, 0.1, 0.3]}\n",
    "\n",
    "# scv = StratifiedKFold(n_splits = 3, random_state = 41)\n",
    "\n",
    "# grid_searcher = GridSearchCV(estimator = plain_catb, scoring = \"roc_auc\" , param_grid = param_grid, cv = scv, verbose=1, n_jobs = -1 )\n",
    "# grid_searcher.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_searcher.best_params_ , grid_searcher.best_score_\n",
    "# # ({'depth': 8, 'l2_leaf_reg': 1, 'learning_rate': 0.1}, 0.7907046866963885)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See the performance on tuned CatBoost\n",
    "# tuned_catb = CatBoostClassifier(iterations = 50, loss_function = \"Logloss\", eval_metric = \"AUC\", verbose = False)\n",
    "\n",
    "# print(\"Sampled training\")\n",
    "# evaluate_auc(tuned_catb, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "# print(\"All training\")\n",
    "# evaluate_auc(tuned_catb, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble LGBM, Logistic Regression, and CatBoost on under-sampled training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We will try to train several model:\n",
    "# # 4 LGBM, 2 Logistic Regression, and 1 CatBoost\n",
    "# # Ensemble all the results\n",
    "\n",
    "# # Best LGBM Result on validation 0.796\n",
    "# # Best Logistic Regression Result on validation 0.7887\n",
    "# # Best CatBoost Result on validation 0.795\n",
    "\n",
    "# # Validation phase\n",
    "\n",
    "# tuned_lgbm = LGBMClassifier(learning_rate = 0.11, num_leaves = 16, max_depth = 6, lambda_l1 = 0.005,random_state = 41)\n",
    "# tuned_logreg = LogisticRegression(C= 5, max_iter= 3000, solver= 'lbfgs', random_state = 41)\n",
    "# tuned_catb = CatBoostClassifier(loss_function = \"Logloss\", eval_metric = \"AUC\", verbose = False)\n",
    "\n",
    "# def predict_val_random_sample(model, i, X_train, X_val, y_train, y_val):\n",
    "#     # Define RandomUnderSampler\n",
    "#     print(\"Random State: {}\".format(i))\n",
    "#     under_sampler = RandomUnderSampler(sampling_strategy = 1, random_state = i)\n",
    "#     X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "#     # Evaluate AUC on validation dataset\n",
    "#     evaluate_auc(model, X_train_resampled, X_val, y_train_resampled, y_val)\n",
    "#     # Training model and predict\n",
    "#     model.fit(X_train_resampled, y_train_resampled)\n",
    "#     # Predict\n",
    "#     y_pred = model.predict_proba(X_val)\n",
    "#     return y_pred[:,1]\n",
    "\n",
    "# def predict_val(model, X_train, X_val, y_train, y_val):\n",
    "#     # Evaluate AUC on validation dataset\n",
    "#     evaluate_auc(model, X_train, X_val, y_train, y_val)\n",
    "#     # Training model and predict\n",
    "#     model.fit(X_train, y_train)\n",
    "#     # Predict\n",
    "#     y_pred = model.predict_proba(X_val)\n",
    "#     return y_pred[:,1]\n",
    "    \n",
    "# # Initiate list for result several predictions\n",
    "# results = []\n",
    "\n",
    "# # Initiate several random state\n",
    "# rands = [1, 30, 41, 70]\n",
    "\n",
    "# for i in range(len(rands)):\n",
    "#     results.append(predict_val_random_sample(tuned_lgbm, rands[i], X_train, X_val, y_train, y_val))\n",
    "\n",
    "# for i in range(len(rands)-2):\n",
    "#     results.append(predict_val_random_sample(tuned_logreg, rands[i+2], X_train, X_val, y_train, y_val ))\n",
    "\n",
    "# results.append(predict_val_random_sample(tuned_catb, rands[3], X_train, X_val, y_train, y_val ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train NN\n",
    "\n",
    "# epochs = 30\n",
    "# learning_rate = 0.001 # initial learning_rate\n",
    "# decay_rate = 0.01\n",
    "# momentum = 0.8\n",
    "\n",
    "# batch_size = int(480000/100)\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import History\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# # Define model architecture\n",
    "\n",
    "# mlp = keras.models.Sequential([\n",
    "#     keras.layers.Dense(64, activation = 'relu', kernel_initializer = 'he_normal'),\n",
    "#     keras.layers.Dropout(0.2),\n",
    "#     keras.layers.Dense(64, activation = 'relu', kernel_initializer = 'he_normal'),\n",
    "#     keras.layers.Dropout(0.2),\n",
    "#     keras.layers.Dense(32, activation = 'relu', kernel_initializer = 'he_normal'),\n",
    "#     keras.layers.Dropout(0.1),\n",
    "#     keras.layers.Dense(1, activation = 'sigmoid')    \n",
    "# ])\n",
    "\n",
    "# def auc_metric(y_true, y_pred):\n",
    "#     def fallback_auc(y_true, y_pred):\n",
    "#         try:\n",
    "#             return roc_auc_score(y_true, y_pred)\n",
    "#         except:\n",
    "#             return 0.5\n",
    "#     return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)\n",
    "\n",
    "\n",
    "# adam = Adam(learning_rate = learning_rate)\n",
    "\n",
    "# # exponential decay for learning rate\n",
    "\n",
    "# def exp_decay(epoch):\n",
    "#     lrate = learning_rate * np.exp(-decay_rate*epoch)\n",
    "#     return lrate\n",
    "\n",
    "# # learning rate schedule callback\n",
    "# loss_history = History()\n",
    "# lr_rate = LearningRateScheduler(exp_decay)\n",
    "# callbacks_list = [loss_history, lr_rate]\n",
    "\n",
    "# # Compile mlp\n",
    "# mlp.compile(optimizer = adam, loss= 'binary_crossentropy', metrics = [auc_metric])\n",
    "\n",
    "# lr_model_history = mlp.fit(X_train, y_train.to_numpy(),\n",
    "#         batch_size = batch_size,\n",
    "#         epochs = epochs,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         callbacks = callbacks_list)\n",
    "\n",
    "# results.append(mlp.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensemble the prediction with rank-based weighted averaging\n",
    "\n",
    "# # Rank all the predictions using dense method\n",
    "# ranks = []\n",
    "# for i in range(len(results)):\n",
    "#     ranks.append( scipy.stats.rankdata( results[i] , method = 'dense') )\n",
    "\n",
    "# # Weighted average ranking\n",
    "# avg_rank = np.average( ranks, axis = 0, weights = [1, 1, 1, 1, 1, 1, 1, 1] )\n",
    "\n",
    "# # Scale the average rank to 0-1\n",
    "# final_result = (avg_rank - avg_rank.min()) / (avg_rank.max() - avg_rank.min())\n",
    "\n",
    "# score = roc_auc_score(y_val,final_result)\n",
    "# print(\"AUC Score on Validation: {}\".format(score))\n",
    "# # The result better 0.79726 than any other predictions \n",
    "# # Ensemble 4 models: 2 LGBM, 1 LogReg, 1 CatBoost 0.79745; weights = [3, 3, 1, 3]\n",
    "# # Ensemble 4 models: 2 LGBM, 1 LogReg, 1 CatBoost, and 1 plain CatBoost 0.79767 ; weights = [3, 3, 1, 3, 3]\n",
    "# # Revert to first ensemble\n",
    "# # After adding to ~ 200 features AUC Score on Validation: 0.7969246358526689 weights = [3, 3, 3, 3, 1, 1, 2]\n",
    "# # AUC Score on Validation: 0.7971906169472329 ; weights = [1, 1, 1, 1, 1, 1, 1]\n",
    "# # AUC Score on Validation: 0.797321531961379\n",
    "# # AUC Score on Validation: 0.7973375349368386 ; weights = [2, 2, 2, 1 , 1, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State: 1\n",
      "Start Training\n",
      "Start Predicting\n",
      "Random State: 30\n",
      "Start Training\n",
      "Start Predicting\n",
      "Random State: 41\n",
      "Start Training\n",
      "Start Predicting\n",
      "Random State: 70\n",
      "Start Training\n",
      "Start Predicting\n",
      "Random State: 30\n",
      "Start Training\n",
      "Start Predicting\n",
      "Random State: 41\n",
      "Start Training\n",
      "Start Predicting\n",
      "Random State: 41\n",
      "Start Training\n",
      "Start Predicting\n"
     ]
    }
   ],
   "source": [
    "# We will try to train several model:\n",
    "# 4 LGBM, 2 Logistic Regression, and 2 CatBoost\n",
    "# Ensemble all the results\n",
    "\n",
    "# Best LGBM Result on validation 0.796\n",
    "# Best Logistic Regression Result on validation 0.7887\n",
    "# Best CatBoost Result on validation 0.795\n",
    "\n",
    "# Training all data phase\n",
    "tuned_lgbm = LGBMClassifier(learning_rate = 0.11, num_leaves = 16, max_depth = 6, lambda_l1 = 0.005,random_state = 41)\n",
    "tuned_logreg = LogisticRegression(C= 5, max_iter= 3000, solver= 'lbfgs', random_state = 41)\n",
    "tuned_catb = CatBoostClassifier(loss_function = \"Logloss\", eval_metric = \"AUC\", verbose = False)\n",
    "\n",
    "# Training on all dataset and predict test\n",
    "def predict_test_random_sample(model, i, X_train, X_test, y_train):\n",
    "    # Define RandomUnderSampler\n",
    "    print(\"Random State: {}\".format(i))\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy = 1, random_state = i)\n",
    "    X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "    print(\"Start Training\")\n",
    "    # Training model and predict\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    # Predict\n",
    "    print(\"Start Predicting\")\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    return y_pred[:,1]\n",
    "\n",
    "def predict_test(model, X_train, X_test, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    return y_pred[:,1]\n",
    "\n",
    "# Initiate list for result several predictions\n",
    "results = []\n",
    "\n",
    "# Initiate several random state\n",
    "rands = [1, 30, 41, 70]\n",
    "\n",
    "# Training on all training dataset - LGBM\n",
    "for i in range(len(rands)):\n",
    "    results.append( predict_test_random_sample(tuned_lgbm, rands[i], train, test, train_target) )\n",
    "\n",
    "for i in range(len(rands)-2):\n",
    "    results.append( predict_test_random_sample(tuned_logreg, rands[i+1], train, test, train_target) )\n",
    "\n",
    "results.append( predict_test_random_sample(tuned_catb, rands[2], train, test, train_target) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples\n",
      "Epoch 1/30\n",
      "Epoch 2/30\n",
      "Epoch 3/30\n",
      "Epoch 4/30\n",
      "Epoch 5/30\n",
      "Epoch 6/30\n",
      "Epoch 7/30\n",
      "Epoch 8/30\n",
      "Epoch 9/30\n",
      "Epoch 10/30\n",
      "Epoch 11/30\n",
      "Epoch 12/30\n",
      "Epoch 13/30\n",
      "Epoch 14/30\n",
      "Epoch 15/30\n",
      "Epoch 16/30\n",
      "Epoch 17/30\n",
      "Epoch 18/30\n",
      "Epoch 19/30\n",
      "Epoch 20/30\n",
      "Epoch 21/30\n",
      "Epoch 22/30\n",
      "Epoch 23/30\n",
      "Epoch 24/30\n",
      "Epoch 25/30\n",
      "Epoch 26/30\n",
      "Epoch 27/30\n",
      "Epoch 28/30\n",
      "Epoch 29/30\n",
      "Epoch 30/30\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "learning_rate = 0.001 # initial learning_rate\n",
    "decay_rate = 0.01\n",
    "momentum = 0.8\n",
    "\n",
    "batch_size = int(480000/100)\n",
    "\n",
    "# exponential decay for learning rate\n",
    "def exp_decay(epoch):\n",
    "    lrate = learning_rate * np.exp(-decay_rate*epoch)\n",
    "    return lrate\n",
    "\n",
    "def auc_metric(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)\n",
    "\n",
    "mlp = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation = 'relu', kernel_initializer = 'he_normal'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation = 'relu', kernel_initializer = 'he_normal'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(32, activation = 'relu', kernel_initializer = 'he_normal'),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')    \n",
    "])\n",
    "\n",
    "adam = Adam(learning_rate = learning_rate)\n",
    "\n",
    "# learning rate schedule callback\n",
    "loss_history = History()\n",
    "lr_rate = LearningRateScheduler(exp_decay)\n",
    "callbacks_list = [loss_history, lr_rate]\n",
    "\n",
    "# Compile mlp\n",
    "mlp.compile(optimizer = adam, loss= 'binary_crossentropy', metrics = [auc_metric])\n",
    "\n",
    "# Training mlp\n",
    "mlp.fit(train, train_target.to_numpy(),\n",
    "        batch_size = batch_size,\n",
    "        epochs = epochs,\n",
    "        callbacks = callbacks_list,\n",
    "        verbose = -1\n",
    "       )\n",
    "\n",
    "results.append( mlp.predict(test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble the prediction with rank-based weighted averaging\n",
    "\n",
    "# Rank all the predictions\n",
    "ranks = []\n",
    "\n",
    "for i in range(len(results)):\n",
    "    ranks.append( scipy.stats.rankdata(results[i], method = 'dense') )\n",
    "\n",
    "# Average the rank\n",
    "avg_rank = np.average(ranks, axis = 0, weights = [1, 1, 1, 1, 1, 1, 1, 1] )\n",
    "\n",
    "# Scale the average rank to 0-1\n",
    "final_result = (avg_rank - avg_rank.min()) / (avg_rank.max() - avg_rank.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make file for submission\n",
    "exp9_submission = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/sample_submission.csv')\n",
    "exp9_submission['target'] = final_result\n",
    "exp9_submission.to_csv('ensemble_lgbm_logreg_catboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best Model perform 0.7825\n",
    "# # We will try to train several model:\n",
    "# # 4 LGBM, 2 Logistic Regression, and 2 CatBoost\n",
    "# # Ensemble all the results\n",
    "\n",
    "# # Training all data phase\n",
    "\n",
    "# tuned_lgbm = LGBMClassifier(n_estimators = 300, learning_rate = 0.1, num_leaves = 6, max_depth = 5,random_state = 41)\n",
    "# tuned_logreg = LogisticRegression(random_state = 41)\n",
    "# tuned_catb = CatBoostClassifier(loss_function = \"Logloss\", eval_metric = \"AUC\", verbose = False)\n",
    "\n",
    "# # Training on all dataset and predict test\n",
    "# def predict_test_random_sample(model, i, X_train, X_test, y_train):\n",
    "#     # Define RandomUnderSampler\n",
    "#     print(\"Random State: {}\".format(i))\n",
    "#     under_sampler = RandomUnderSampler(sampling_strategy = 1, random_state = i)\n",
    "#     X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "#     print(\"Start Training\")\n",
    "#     # Training model and predict\n",
    "#     model.fit(X_train_resampled, y_train_resampled)\n",
    "#     # Predict\n",
    "#     print(\"Start Predicting\")\n",
    "#     y_pred = model.predict_proba(X_test)\n",
    "#     return y_pred[:,1]\n",
    "\n",
    "# def predict_test(model, X_train, X_test, y_train):\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict_proba(X_test)\n",
    "#     return y_pred[:,1]\n",
    "\n",
    "# # Initiate list for result several predictions\n",
    "# results = []\n",
    "\n",
    "# # Initiate several random state\n",
    "# rands = [1, 30, 41, 70]\n",
    "\n",
    "# # Training on all training dataset - LGBM\n",
    "# for i in range(len(rands)):\n",
    "#     results.append( predict_test_random_sample(tuned_lgbm, rands[i], train, test, train_target) )\n",
    "\n",
    "# for i in range(len(rands)-2):\n",
    "#     results.append( predict_test_random_sample(tuned_logreg, rands[i+1], train, test, train_target) )\n",
    "\n",
    "# results.append( predict_test_random_sample(tuned_catb, rands[2], train, test, train_target) )\n",
    "\n",
    "\n",
    "# # Ensemble the prediction with rank-based weighted averaging\n",
    "\n",
    "# # Rank all the predictions\n",
    "# ranks = []\n",
    "\n",
    "# for i in range(len(results)):\n",
    "#     ranks.append( scipy.stats.rankdata(results[i], method = 'dense') )\n",
    "\n",
    "# # Average the rank\n",
    "# avg_rank = np.average(ranks, axis = 0, weights = [4, 4, 4, 4, 1, 1, 4] )\n",
    "\n",
    "# # Scale the average rank to 0-1\n",
    "# final_result = (avg_rank - avg_rank.min()) / (avg_rank.max() - avg_rank.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Model\n",
    "CatBoost is a gradient boosting solution which is considered as the state of the art for tabular data with categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define CatBoost model - without onehot encoding features\n",
    "# catb = CatBoostClassifier(iterations = 100 , learning_rate = 0.3, loss_function= \"Logloss\", eval_metric = \"AUC\", verbose=False)\n",
    "# catb.fit(train, train_target)\n",
    "\n",
    "# # Predict\n",
    "# y_pred = catb.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make file for submission\n",
    "# exp7_submission = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/sample_submission.csv')\n",
    "# exp7_submission['target'] = y_pred[:,1]\n",
    "# exp7_submission.to_csv('plain_catboost_without_onehot_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensional Reduction / Under Sampling\n",
    "I found that current dataset 600k x 5769 features took a lot of memory\n",
    "In this section I will try several methods for reducing memory burden by dimensionality reduction (PCA) or under-sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of positive labels on training data\n",
    "# train_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA for dimensional reduction\n",
    "\n",
    "# # Lets build PCA that will retain 95% variance on the data\n",
    "# pca = TruncatedSVD(n_components=100)\n",
    "# pca.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plotting the Cumulative Summation of the Explained Variance\n",
    "# plt.figure()\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Variance (%)') #for each component\n",
    "# plt.title('Data Explained Variance')\n",
    "# plt.axhline(y=0.95, color = 'r', linestyle='--')\n",
    "# plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize = 12)\n",
    "# plt.show()\n",
    "\n",
    "# # Based on the picture, after 70 components there are no significant increment of vairance % of total data\n",
    "# # I decide to only use 70 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets build PCA that will retain 95% variance on the data\n",
    "# pca = TruncatedSVD(n_components=70, random_state = 41)\n",
    "# pca.fit(train_w_onehot)\n",
    "\n",
    "# train_reduced = pca.transform(train_w_onehot)\n",
    "# test_reduced = pca.transform(test_w_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define CatBoost model - without onehot encoding features\n",
    "# catb = CatBoostClassifier(iterations = 100 , learning_rate = 0.3, loss_function= \"Logloss\", eval_metric = \"AUC\", verbose=False)\n",
    "# catb.fit(train, train_target)\n",
    "\n",
    "# # Predict\n",
    "# y_pred = catb.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make file for submission\n",
    "# exp8_submission = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/sample_submission.csv')\n",
    "# exp8_submission['target'] = y_pred[:,1]\n",
    "# exp8_submission.to_csv('plain_catboost_reduced_model.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
